{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, data as gdata, nn\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5. Exercises\n",
    "\n",
    "1. **Assume that we have some data  $ùë•_1,‚Ä¶ ùë•_ùëõ \\in ‚Ñù$ . Our goal is to find a constant  ùëè  such that $‚àë_ùëñ (ùë•ùëñ‚àíùëè)^2$  is minimized.**\n",
    "\n",
    "    a. **Find the optimal closed form solution.**\n",
    "\n",
    "    b. **What does this mean in terms of the Normal distribution?**\n",
    "    \n",
    "    \n",
    "2. **Assume that we want to solve the optimization problem for linear regression with quadratic loss explicitly in closed form. To keep things simple, you can omit the bias  ùëè  from the problem.**\n",
    "    a. **Rewrite the problem in matrix and vector notation (hint - treat all the data as a single matrix).**\n",
    "\n",
    "    b. **Compute the gradient of the optimization problem with respect to  ùë§ .**\n",
    "    \n",
    "    c. **Find the closed form solution by solving a matrix equation.**\n",
    "    \n",
    "    d. **When might this be better than using stochastic gradient descent (i.e. the incremental optimization approach that we discussed above)? When will this break (hint - what happens for high-dimensional  ùë• , what if many observations are very similar)?.**\n",
    "    \n",
    "    \n",
    "3. **Assume that the noise model governing the additive noise  ùúñ  is the exponential distribution. That is,  $ùëù(ùúñ)=12exp(‚àí|ùúñ|)$.**\n",
    "\n",
    "    a.**Write out the negative log-likelihood of the data under the model  $‚àílogùëù(ùëå|ùëã)$.**\n",
    "    \n",
    "    b. **Can you find a closed form solution?**\n",
    "    \n",
    "    c. **Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?**\n",
    "    \n",
    "    \n",
    "4. **Compare the runtime of the two methods of adding two vectors using other packages (such as NumPy) or other programming languages (such as MATLAB).**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.9. Exercises\n",
    "\n",
    "1. **What would happen if we were to initialize the weights ùê∞=0 . Would the algorithm still work?**\n",
    "\n",
    "    It doesn't work. Each neuron at a layer will have exactly same gradient at each update, hence all the neurons will learn the same thing.\n",
    "    \n",
    "\n",
    "2. **Assume that you‚Äôre Georg Simon Ohm trying to come up with a model between voltage and current. Can you use autograd to learn the parameters of your model.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1  # Learning rate\n",
    "num_epochs = 10  # Number of iterations\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "b = nd.zeros(shape=(1,))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(nd.dot(voltage, w) + b, current)  # Minibatch loss \n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Can you use Planck‚Äôs Law to determine the temperature of an object using spectral energy density.**\n",
    "\n",
    "\n",
    "4. **What are the problems you might encounter if you wanted to extend autograd to second derivatives? How would you fix them?**\n",
    "\n",
    "    The second order gradient (Jacobian matrix) will have $n^2$ more parameters for a layer with n neuron weights.\n",
    "\n",
    "5. **Why is the reshape function needed in the squared_loss function?**\n",
    "\n",
    "\n",
    "\n",
    "6. **Experiment using different learning rates to find out how fast the loss function value drops.**\n",
    "\n",
    "\n",
    "7. **If the number of examples cannot be divided by the batch size, what happens to the data_iter function‚Äôs behavior?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
