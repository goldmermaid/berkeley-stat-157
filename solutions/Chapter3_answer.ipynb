{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3 of D2L Textbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, data as gdata, nn\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5. Exercises\n",
    "\n",
    "1. **Assume that we have some data  $x_1,â€¦ x_n \\in \\mathbb{R}$ . Our goal is to find a constant  ğ‘  such that $\\sum_i (x_i - b)^2$  is minimized.**\n",
    "\n",
    "    a.**Find the optimal closed form solution.**\n",
    "    \n",
    "    b.**What does this mean in terms of the Normal distribution?**\n",
    "    \n",
    "$$\\frac{\\partial}{\\partial b}{\\sum_i (x_i - b)^2} = - 2 \\sum_i (x_i - b) = 0  \\textbf{  =>  } \\hat{b} = \\bar{x} = mean(x_i)$$\n",
    "$$\\frac{\\partial}{\\partial b}{\\sum_i (x_i - b)^2} = 2 > 0  \\textbf{  =>  } \\hat{b} \\text{ is a minimum.  }$$\n",
    "\n",
    "   This equals to the mean ($\\mu$) of the normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "2. **Assume that we want to solve the optimization problem for linear regression with quadratic loss explicitly in closed form. To keep things simple, you can omit the bias  ğ‘  from the problem.**\n",
    "\n",
    "    a. **Rewrite the problem in matrix and vector notation (hint - treat all the data as a single matrix).**\n",
    "\n",
    "    $$L(\\mathbf{w}) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} - y^{(i)}\\right)^2. = ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2$$\n",
    "    \n",
    "    b. **Compute the gradient of the optimization problem with respect to  ğ‘¤ .**\n",
    "    \n",
    "    $$\\frac{\\partial}{\\partial \\mathbf{w}}{L(\\mathbf{w})} = 2 X^T (X\\mathbf{w} - \\mathbf{y})$$\n",
    "    \n",
    "    c. **Find the closed form solution by solving a matrix equation.**\n",
    "    \n",
    "    $$\\mathbf{\\hat{w}} = (X^TX)^{-1} X^T \\mathbf{y}$$\n",
    "    \n",
    "    d. **When might this be better than using stochastic gradient descent (i.e. the incremental optimization approach that we discussed above)? When will this break (hint - what happens for high-dimensional  ğ‘¥ , what if many observations are very similar)?.**\n",
    "    \n",
    "    When the features are linear independent.\n",
    "    \n",
    "    For high demensional x, if some of the features are of high correlations, then $(X^T X)^{-1}$ may not existed.\n",
    "    \n",
    "    \n",
    "3. **Assume that the noise model governing the additive noise  ğœ–  is the exponential distribution. That is,  $p(\\epsilon)=\\frac{1}{2}exp(âˆ’|\\epsilon|)$.**\n",
    "\n",
    "    a.**Write out the negative log-likelihood of the data under the model  $âˆ’log p(Y|X)$.**\n",
    "    \n",
    "    $$âˆ’log p(Y|X) = -log(\\frac{1}{2} exp(-|X\\mathbf{w}-Y|)) = |X\\mathbf{w}-Y| - log(\\frac{1}{2}) $$\n",
    "    \n",
    "    b. **Can you find a closed form solution?**\n",
    "    \n",
    "    $$ \\mathbf{w} = argmin_{\\mathbf{w}} |X\\mathbf{w}-Y| $$\n",
    "    \n",
    "    c. **Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?**\n",
    "    \n",
    "    \n",
    "4. **Compare the runtime of the two methods of adding two vectors using other packages (such as NumPy) or other programming languages (such as MATLAB).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.9. Exercises\n",
    "\n",
    "1. **What would happen if we were to initialize the weights ğ°=0 . Would the algorithm still work?**\n",
    "\n",
    "    It doesn't work. Each neuron at a layer will have exactly same gradient at each update, hence all the neurons will learn the same thing.\n",
    "    \n",
    "\n",
    "2. **Assume that youâ€™re Georg Simon Ohm trying to come up with a model between voltage and current. Can you use autograd to learn the parameters of your model.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "epoch 1, loss 0.477460\n",
      "epoch 2, loss 0.597413\n",
      "epoch 3, loss 0.385625\n",
      "epoch 4, loss 0.644568\n",
      "epoch 5, loss 0.315570\n",
      "epoch 6, loss 0.455128\n",
      "epoch 7, loss 0.450721\n",
      "epoch 8, loss 0.501864\n",
      "epoch 9, loss 0.325878\n",
      "epoch 10, loss 0.545352\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1  # Learning rate\n",
    "num_epochs = 10  # Number of iterations\n",
    "batch_size = 10\n",
    "num_examples = 1000\n",
    "\n",
    "## construct voltage, current & resistance\n",
    "resistance = 10 # nd.random.normal(scale=0.01, shape=(num_examples, 1))\n",
    "current = nd.random.normal(scale=1, shape=(num_examples, 1))\n",
    "voltage = resistance * current\n",
    "print(voltage.shape)\n",
    "voltage += nd.random.normal(scale=0.01, shape=(num_examples, 1))\n",
    "\n",
    "## initialize parameters                            \n",
    "loss = gloss.L2Loss()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.1))\n",
    "trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd', {'learning_rate': lr, 'wd': 1})\n",
    "trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd', {'learning_rate': lr})\n",
    "                            \n",
    "for epoch in range(num_epochs):\n",
    "    b = nd.zeros(shape=(1))\n",
    "    w = nd.random.normal(scale=0.01, shape=(1))\n",
    "    for X, y in data_iter(batch_size, current, voltage):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)  # Minibatch loss\n",
    "#         import ipdb\n",
    "#         ipdb.set_trace()\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        trainer_w.step(batch_size)\n",
    "        trainer_b.step(batch_size)\n",
    "    train_l = loss(linreg(voltage, w, b), current)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Can you use Planckâ€™s Law to determine the temperature of an object using spectral energy density.**\n",
    "\n",
    "    ??????\n",
    "    \n",
    "\n",
    "4. **What are the problems you might encounter if you wanted to extend autograd to second derivatives? How would you fix them?**\n",
    "\n",
    "    The second order gradient (Jacobian matrix) will have $n^2$ more parameters for a layer with n neuron weights.\n",
    "    \n",
    "\n",
    "5. **Why is the reshape function needed in the squared_loss function?**\n",
    "\n",
    "    Since the prediction tensor can have arbitrary shape, while its size(the product of all the dimension times together) is equal to true label \"y\", hence we just need to reshape true label \"y\" as same as the shape of prediction y.\n",
    "    \n",
    "\n",
    "6. **Experiment using different learning rates to find out how fast the loss function value drops.**\n",
    "\n",
    "    A good start choose of lr is 0.1. Sometimes 1 is too high and the model cannot converge, but 0.01 is too small and the model converge slowly.\n",
    "    \n",
    "\n",
    "7. **If the number of examples cannot be divided by the batch size, what happens to the data_iter functionâ€™s behavior?**\n",
    "\n",
    "    It will take the last few examples which are less than a batchsize to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.9. Exercises\n",
    "\n",
    "\n",
    "1. **If we replace l = loss(output, y) with l = loss(output, y).mean(), we need to change trainer.step(batch_size) to trainer.step(1) accordingly. Why?**\n",
    "\n",
    "    From the documentation of `Gluon.Trainer.step`, gradient will be normalized by 1/batch_size, which is the same as take a mean with batch_size 1.\n",
    "    \n",
    "    See details: https://mxnet.apache.org/api/python/gluon/gluon.html#mxnet.gluon.Trainer\n",
    "\n",
    "\n",
    "2. **Review the MXNet documentation to see what loss functions and initialization methods are provided in the modules gluon.loss and init. Replace the loss by Huberâ€™s loss.**\n",
    "\n",
    "    L1Loss, L2Loss, SigmoidBinaryCrossEntropyLoss, SoftmaxCrossEntropyLoss, KLDivLoss, etc.\n",
    "    https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.Loss\n",
    "    \n",
    "\n",
    "3. **How do you access the gradient of dense.weight?**\n",
    "\n",
    "    `with autograd.record():` records computation history on the fly to calculate gradients later.\n",
    "    \n",
    "    To compute gradient with respect to an NDArray x, first call `x.attach_grad()` to allocate space for the gradient. Then, start a with `autograd.record():` block, and do some computation. Finally, call `backward()` on the result. See details https://mxnet.apache.org/api/python/autograd/autograd.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2. 4. 6. 8.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = mx.nd.array([1,2,3,4])\n",
    "x.attach_grad()\n",
    "with mx.autograd.record():\n",
    "    y = x * x + 1\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.6. Exercises\n",
    "\n",
    "1. **Show that the Kullback-Leibler divergence  $ğ·(ğ‘â€–ğ‘)$  is nonnegative for all distributions  ğ‘  and  ğ‘ . Hint - use Jensenâ€™s inequality, i.e. use the fact that  $âˆ’logğ‘¥$  is a convex function.**\n",
    "\n",
    "    Since $-logx$ is convex, then\n",
    "\n",
    "    $$D(p\\|q) = -\\sum_j p(j) \\log q(j) - H[p] = \\sum_j p(j) \\log \\frac{p(j)}{q(j)} = \\sum_j - p(j) \\log \\frac{q(j)}{p(j)} \\geq  -\\log (\\sum_j p(j) \\frac{q(j)}{p(j)}) = 0$$\n",
    "\n",
    "2. **Show that  $ log \\sum j \\exp(o_j)$  is a convex function in o.**\n",
    "\n",
    "    For any j > 1, $f_j (x) = x^j$ is a convex function. hence, \n",
    "    $$f_j (\\sum_o o_j) \\leq \\sum_o f_j(o_j) => (\\sum_o o_j)^j \\leq \\sum_o (o_j)^j$$\n",
    "    \n",
    "    $$i.e. \\log j \\exp(\\sum_o o_j) \\leq \\sum_o log j \\exp o_j$$\n",
    "    \n",
    "    Hence, $ log \\sum j \\exp(o_j)$  is a convex function in o.\n",
    "    \n",
    "    \n",
    "3. **We can explore the connection between exponential families and the softmax in some more depthã€‚**\n",
    "\n",
    "    a. **Compute the second derivative of the cross entropy loss  ğ‘™(ğ‘¦,ğ‘¦Ì‚)  for the softmax.**\n",
    "    \n",
    "    By definition of softmax, \n",
    "    $$softmax(y, \\hat{y}) = -\\sum_j y_j \\log \\hat{y}_j = \\sum_j y_j \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j = \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j$$\n",
    "    \n",
    "    Hence, the derivative is\n",
    "    $$\\partial_{o_j} softmax(y, \\hat{y}) = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} - y_j = \\mathrm{softmax}(o_j) - y_j = \\hat{y_j} - y_j = \\Pr(y = j|x) - y_j$$\n",
    "    \n",
    "    b. **Compute the variance of the distribution given by  softmax(ğ‘œ)  and show that it matches the second derivative computed above.**\n",
    "    \n",
    "    For any exponential distribution family function, the first derivative of the cumulant function is mean, while second derivative is the variance of the corresponding distribution.\n",
    "    \n",
    "    ![title](image/textbook_solution_4.4.3.png)\n",
    "    \n",
    "<img src=\"image/textbook_solution_4.4.3.png\" alt=\"Drawing\" style=\"width: 20px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Assume that we three classes which occur with equal probability, i.e. the probability vector is  ($\\frac{1}{3}$, $\\frac{1}{3}$, $\\frac{1}{3}$).**\n",
    "\n",
    "    a. **What is the problem if we try to design a binary code for it? Can we match the entropy lower bound on the number of bits?**\n",
    "    \n",
    "    `H(p) = 3*np.log2(3) = np.log2(27) ~= 4.75488`, However, we will need 5 bits to encode.\n",
    "    \n",
    "    b. **Can you design a better code. Hint - what happens if we try to encode two independent observations? What if we encode  ğ‘›  observations jointly?**\n",
    "    \n",
    "    \n",
    "    \n",
    "5. **Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as $\\text{RealSoftMax(ğ‘,ğ‘)}=log(exp(ğ‘)+exp(ğ‘))$.**\n",
    "\n",
    "    a. **Prove that  RealSoftMax(ğ‘,ğ‘)>max(ğ‘,ğ‘).**\n",
    "    \n",
    "    $$RealSoftMax(ğ‘,ğ‘) = log(exp(ğ‘)+exp(ğ‘)) > log(exp(max(a,b))) = max(ğ‘,ğ‘)$$\n",
    "    \n",
    "    b. **Prove that this holds for  $ğœ†^{âˆ’1} \\text{RealSoftMax}(ğœ†ğ‘,ğœ†ğ‘)$ , provided that ğœ†>0.**\n",
    "\n",
    "    $$ğœ†^{âˆ’1} \\text{RealSoftMax}(ğœ†ğ‘,ğœ†ğ‘) = ğœ†^{âˆ’1} log(exp(ğœ†ğ‘)+exp(ğœ†ğ‘)) > ğœ†^{âˆ’1} log(exp(max(ğœ†ğ‘,ğœ†ğ‘))) = max(a,b)$$\n",
    "    \n",
    "    c. **Show that for  ğœ†â†’âˆ  we have  $ğœ†^{âˆ’1} \\text{RealSoftMax}(ğœ†ğ‘,ğœ†ğ‘)â†’max(ğ‘,ğ‘)$**.\n",
    "    \n",
    "    By Taylor expansion, $log(X+a) \\simeq log(X) + a/X$, \n",
    "    \n",
    "    Suppose $max(ğ‘,ğ‘) = a$, then,\n",
    "    \n",
    "    $$ ğœ†^{âˆ’1} log(exp(ğœ†ğ‘)+exp(ğœ†ğ‘)) \\simeq ğœ†^{âˆ’1} log(exp(ğœ†a)) + \\frac{exp(ğœ†ğ‘)}{exp(ğœ†a)} \\simeq ğœ†^{âˆ’1} log(exp(ğœ†a)) = max(ğ‘,ğ‘)$$\n",
    "    \n",
    "    d. **What does the soft-min look like?**\n",
    "    \n",
    "    For $\\alpha < 0, softmin_{\\alpha}(x_k) â†’ min{x_k}$ as $\\alpha â†’ -âˆ$, where \n",
    "    \n",
    "    $$softmin_{\\alpha}(x_k) = \\frac{\\sum_k x_k \\exp(\\alpha x_k)}{\\sum_k \\exp(\\alpha x_k)}$$\n",
    "    \n",
    "    e. **Extend this to more than two numbers.**\n",
    "    \n",
    "    As above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4. Exercises\n",
    "\n",
    "1. **Does reducing batch_size (for instance, to 1) affect performance?**\n",
    "\n",
    "    Update at each gradient descent (very noisy and may go off in a direction far from the minibatch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minimal.)\n",
    "    \n",
    "    \n",
    "2. **For non-Windows users, try modifying num_workers to see how it affects read performance.**\n",
    "\n",
    "    num_workers (default 0) is the number of multiprocessing workers to use for data preprocessing.\n",
    "    \n",
    "    \n",
    "3. **Use the MXNet documentation to see which other datasets are available in mxnet.gluon.data.vision.**\n",
    "\n",
    "    https://mxnet.incubator.apache.org/api/python/gluon/data.html#vision\n",
    "    \n",
    "4. **Use the MXNet documentation to see which other transformations are available in mxnet.gluon.data.vision.transforms.**\n",
    "\n",
    "    https://mxnet.incubator.apache.org/api/python/gluon/data.html#vision-transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.9. Exercises\n",
    "\n",
    "1. **In this section, we directly implemented the softmax function based on the mathematical definition of the softmax operation. What problems might this cause (hint - try to calculate the size of  exp(50) )?**\n",
    "\n",
    "    exp(50) may lead to numerical overflow (when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits).\n",
    "    \n",
    "    \n",
    "\n",
    "2. **The function cross_entropy in this section is implemented according to the definition of the cross-entropy loss function. What could be the problem with this implementation (hint - consider the domain of the logarithm)?**\n",
    "\n",
    "    The logarithm assume a positive input domain, hence we need to make sure the $\\hat{y}$ is a strictly postive vector.\n",
    "\n",
    "3. **What solutions you can think of to fix the two problems above?**\n",
    "\n",
    "    We can reimplement the formula as:\n",
    "    $$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(X_{ij} - b )  \\exp(b)}{\\sum_k \\exp(X_{ik} - b)  \\exp(b)} = \\frac{ \\exp(X_{ij} - b) }{ \\sum_{k=1}^n \\exp(X_{ik} - b) },$$ where, $b = \\max_{k=1}^n x_k$.\n",
    "\n",
    "\n",
    "4. **Is it always a good idea to return the most likely label. E.g. would you do this for medical diagnosis?**\n",
    "\n",
    "    No. For dataset with extremely unbalanced classes, if we evaluate the model accuracy by having same penalty on each sample (rather than each class), the model may turn to return the most likely label all the time. For example, in medical diagnosis of rear disease, we have to punish more on wrong prediction of potisive (disease) class.\n",
    "    \n",
    "\n",
    "5. **Assume that we want to use softmax regression to predict the next word based on some features. What are some problems that might arise from a large vocabulary?**\n",
    "\n",
    "    For a large vocabulary, the choose space of next word will be large, and hence the matrix will be sparse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.5. Exercises\n",
    "\n",
    "1. **Try adjusting the hyper-parameters, such as batch size, epoch, and learning rate, to see what the results are.**\n",
    "\n",
    "    Larger batch size => Lack of ability to generalize => \"saddle point\"\n",
    "    \n",
    "    Larger epoch => The more times that the model goes through the dataset => best accuracy in general\n",
    "    \n",
    "    Learning rate(lr):\n",
    "    \n",
    "    a. If lr too large, the model may oscillate around optimal point, or even get diverged in the end. \n",
    "    b. If lr too small, it may take too long to train. Even though the loss and train accuracy continues improving at a slow pace, since the gradient is too small at each step.\n",
    "\n",
    "\n",
    "2. **Why might the test accuracy decrease again after a while? How could we fix this?**\n",
    "\n",
    "    If the model get overfitted with training set, it may not be able to generalize to test set. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
