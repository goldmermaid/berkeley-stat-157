{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D2L Textbook Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, data as gdata, nn\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4\n",
    "#### 4.1.4. Exercises\n",
    "\n",
    "1. **Compute the derivative of the tanh and the pReLU activation function.**\n",
    "\n",
    "    a. The derivative of the Tanh function is:\n",
    "    $$\\frac{d}{dx} \\mathrm{tanh}(x) = 1 - \\mathrm{tanh}^2(x).$$\n",
    "    b. The derivative of the pReLU function is:    \n",
    "    $$\\mathrm{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)$$\n",
    "    $$\\begin{equation}\n",
    "          \\frac{d}{dx} \\mathrm{pReLU}(x) =\n",
    "                \\begin{cases}\n",
    "                  1 & \\text{if $x > 0$}\\\\\n",
    "                  undefined & \\text{if $x = 0$}\\\\\n",
    "                  \\alpha & \\text{if $x < 0$}\n",
    "                \\end{cases}       \n",
    "        \\end{equation}$$\n",
    "\n",
    "\n",
    "2. **Show that a multilayer perceptron using only ReLU (or pReLU) constructs a continuous piecewise linear function.**\n",
    "\n",
    "    By definiton, a `continous piecewise linear function` is a real-valued function defined on the real numbers, whose graph is composed of continous straight-line sections.\n",
    "    $$\\begin{equation}\n",
    "      \\mathrm{pReLU}(x) =\n",
    "            \\begin{cases}\n",
    "              x & \\text{if $x >= 0$}\\\\\n",
    "              \\alpha x & \\text{if $x < 0$}\n",
    "            \\end{cases}       \n",
    "    \\end{equation}$$\n",
    "    \n",
    "3. **Show that  $tanh(ùë•)+1=2sigmoid(2ùë•)$.**\n",
    "\n",
    "    $$ LHS = \\text{tanh}(x)+1 = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)} + 1\n",
    "                    =\\frac{2}{1 + \\exp(-2x)} .$$\n",
    "    $$ RHS = 2 \\mathrm{sigmoid}(2x) = \\frac{2}{1 + \\exp(-2x)}.$$\n",
    "\n",
    "\n",
    "4. **Assume we have a multilayer perceptron without nonlinearities between the layers. In particular, assume that we have  ùëë  input dimensions,  ùëë  output dimensions and that one of the layers had only  ùëë/2  dimensions. Show that this network is less expressive (powerful) than a single layer perceptron.**\n",
    "\n",
    "    A multilayer perceptron without nonlinearities is equal to one layer perceptron.\n",
    "    $$ {\\hat{\\mathbf{y}}} = \\mathbf{W_2}(\\mathbf{W_1}X + b_1) + b_2 = \\mathbf{W_2} \\mathbf{W_1} X + (\\mathbf{W_2} b_1 + b_2) := \\mathbf{W_3}X + b_3$$\n",
    "    \n",
    "    Hence, if any of layer of d/2 dimension, then the rank of W_3 will be at most d/2, which can not express the final output of dimension d. However, a single layer percptron with sofmax regression will add  nonlinearities to the model, which will learn and represent almost any arbitrary complex function which maps inputs to outputs.\n",
    "    \n",
    "\n",
    "5. **Assume that we have a nonlinearity that applies to one minibatch at a time. What kinds of problems do you expect this to cause?**\n",
    "\n",
    "    Minibatch may not be as representative as whole batch. As a result, parameters learned from the minibatch may get weird gradients and get harder to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7. Exercises¬∂\n",
    "\n",
    "1. **Change the value of the hyper-parameter `num_hiddens` in order to see how this hyperparameter influences your results.**\n",
    "    \n",
    "    The upper bound on the number of hidden neurons that won't result in over-fitting is:\n",
    "    $$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n",
    "\n",
    "where \n",
    "$\\alpha$ = an arbitrary scaling factor usually 2-10;\n",
    "    \n",
    "    $ùëÅ_ùëñ$ = number of input neurons; \n",
    "    $ùëÅ_ùëú$ = number of output neurons;   \n",
    "    $ùëÅ_ùë†$ = number of samples in training data set.\n",
    "    \n",
    "   Below the upper bound, the larger the num_hiddens, the better your results might be.\n",
    "    \n",
    "    \n",
    "2. **Try adding a new hidden layer to see how it affects the results.**\n",
    "\n",
    "    In general, adding a new hidden layer to a shallow networks should improve the accuracy. Since wide and shallow networks are very good at memorization, but not so good at generalization. Multiple layers are much better at generalizing because they learn all the intermediate features between the raw data and the high-level classification.\n",
    "    \n",
    "    \n",
    "3. **How does changing the learning rate change the result.**\n",
    "    \n",
    "    If a learning rate is too high, it may overshoot the minimum and fail to converge in the end. If it is too low, then gradient descent can be slow. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Exercises¬∂\n",
    "\n",
    "1. **Try adding a few more hidden layers to see how the result changes.**\n",
    "\n",
    "2. **Try out different activation functions. Which ones work best?**\n",
    "    \n",
    "    Sigmoid and Tanh both suffers from vanishing gradient problems. \n",
    "    \n",
    "    ReLU rectifies the problem but it could result to \"Dead Neuron\" (since partial of its weights never get updated). Leaky ReLu to fix the problem of dying neurons.  Also, ReLU can be only use within the hidden layer of NN.\n",
    "    \n",
    "    Softmax can be use in the output layers of classification model.\n",
    "    \n",
    "\n",
    "3. **Try out different initializations of the weights.**\n",
    "\n",
    "    Zero initialization. All weights will be the same in the end, since the derivative with respect to loss function is the same.\n",
    "    \n",
    "    Random initialization. Initializing weights randomly, following normal distribution. May suffer from vanishing gradients and exploding gradients.\n",
    "    \n",
    "    Xavier initialization. The initializer fills the weights with random numbers in the range of [‚àíc,c], where $c = \\sqrt{\\frac{3.}{0.5 * (n_{in} + n_{out})}}$. $n_{in}$ is the number of neurons feeding into weights, and $n_{out}$ is the number of neurons the result is fed to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.6. Exercises\n",
    "\n",
    "1. **Can you solve the polynomial regression problem exactly? Hint - use linear algebra.**\n",
    "\n",
    "    Given the polynomial regression samples $\\{(X_i,Y_i)\\}_{i=1}^n$, we have $Y =\\beta_0+\\beta_1X+\\beta_2X^2+\\cdots+\\beta_pX^p+\\varepsilon=\\mathbf{X}\\boldsymbol\\beta+\\varepsilon$\n",
    "        \n",
    "    where\n",
    "    $$\\mathbf{X}=\\pmatrix{\\begin{array}{ccc}1 & X_1 & \\cdots & X_1^p\\\\\n",
    "                \\vdots & \\vdots &\\ddots &\\vdots\\\\\n",
    "                    1 & X_n & \\cdots & X_n^p\n",
    "                \\end{array}}_{n\\times(p+1)}.$$\n",
    "\n",
    "    Hence, $\\hat{\\boldsymbol\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{y},$ where $\\mathbf{y}=(Y_1,\\ldots,Y_n)^T$. \n",
    "    \n",
    "    Note $(ùëù+1)√ó(ùëù+1) = \\mathrm{rank}(\\mathbf{X}^T\\mathbf{X})=\\mathrm{rank}(\\mathbf{X})=n$. Then, if $p=n‚àí1$, $ùêó^T ùêó$ has dimension ùëõ√óùëõ and its rank is ùëõ, so no problem, is invertible. But if $p=n$, the dimension of $ùêó^T ùêó$ is (ùëõ+1)√ó(ùëõ+1) and the rank remains ùëõ, so in that case (and also if $p>n$) is not invertible (linear dependence arises).\n",
    "    \n",
    "    \n",
    "2. **Model selection for polynomials**\n",
    "\n",
    "    a. **Plot the training error vs. model complexity (degree of the polynomial). What do you observe?**\n",
    "\n",
    "    b. **Plot the test error in this case.**\n",
    "\n",
    "    c. **Generate the same graph as a function of the amount of data?**\n",
    "\n",
    "    See example in *4.4.4. Polynomial Regression*.\n",
    "    \n",
    "    \n",
    "\n",
    "3. **What happens if you drop the normalization of the polynomial features  $ùë•_ùëñ$  by  1/ùëñ! . Can you fix this in some other way?**\n",
    "\n",
    "    There might be very large values of gradients and losses, due to very large values for exponents i.\n",
    "\n",
    "\n",
    "4. **What degree of polynomial do you need to reduce the training error to 0?**\n",
    "\n",
    "    As explained in Q1, if $p=n‚àí1$, $ùêó^T ùêó$ has dimension ùëõ√óùëõ and its rank is ùëõ. Then $\\hat{\\boldsymbol\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{y},$ is the exact answer and hence the training error is 0.\n",
    "    \n",
    "\n",
    "5. **Can you ever expect to see 0 generalization error?**\n",
    "\n",
    "    Yes. Sometimes, if we accidentally have training set including all testing set's features and labels, (i.e. testing set items are all duplicated to training set). Then we may see a 0 generalization error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.6. Exercises\n",
    "\n",
    "1. **Experiment with the value of  ùúÜ  in the estimation problem in this page. Plot training and test accuracy as a function of  ùúÜ . What do you observe?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_lambda(wd_list, num_epochs):\n",
    "    '''\n",
    "    wd_list : a list of number which represents weight_decay value\n",
    "    '''\n",
    "    train_ls, test_ls = [], []\n",
    "    for wd in wd_list:\n",
    "        net = nn.Sequential()\n",
    "        net.add(nn.Dense(1))\n",
    "        net.initialize(init.Normal(sigma=0.1))\n",
    "        trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',\n",
    "                                  {'learning_rate': lr, 'wd': wd})\n",
    "        trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',\n",
    "                                  {'learning_rate': lr})\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            for X, y in train_iter:\n",
    "                with autograd.record():\n",
    "                    l = loss(net(X), y)\n",
    "                l.backward()\n",
    "                # Call the step function on each of the two Trainer instances to\n",
    "                # update the weight and bias separately\n",
    "                trainer_w.step(batch_size)\n",
    "                trainer_b.step(batch_size)\n",
    "        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())\n",
    "        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())\n",
    "    d2l.semilogy(wd_list, train_ls, 'weight_decay', 'loss',\n",
    "                     wd_list, test_ls, ['train', 'test'])\n",
    "    \n",
    "fit_and_plot_lambda(wd_list=range(10), num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Use a validation set to find the optimal value of  ùúÜ . Is it really the optimal value? Does this matter?**\n",
    "\n",
    "    Given different hyperparameters, network architecture and dataset, the optimal weight decay will vary. Hence there is no global optimal value. \n",
    "\n",
    "\n",
    "3. **What would the update equations look like if instead of  $‚Äñùê∞‚Äñ^2$  we used  $‚àë_ùëñ|ùë§_ùëñ|$  as our penalty of choice (this is called  ‚Ñì1  regularization).**\n",
    "\n",
    "    For L2, the loss function and corresponding stochastic gradient descent updates is :\n",
    "    $$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
    "    $$\\begin{aligned}\n",
    "w & \\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
    "\\end{aligned}$$\n",
    "\n",
    "    For L1, the loss function and corresponding stochastic gradient descent updates is :\n",
    "    $$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} ‚àë_ùëñ|ùë§_ùëñ|$$\n",
    "    $$\\begin{aligned}\n",
    "w & \\leftarrow \\left(1- \\frac{\\eta\\lambda}{2 |\\mathcal{B}| \\mathbf{|w|}} \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "4. **We know that  $‚Äñùê∞‚Äñ^2=ùê∞^‚ä§ùê∞$ . Can you find a similar equation for matrices (mathematicians call this the Frobenius norm)?**\n",
    "\n",
    "    ![title](image/textbook_solution_4.5.6.png)\n",
    "    \n",
    "    \n",
    "5. **Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways can you think of to deal with overfitting?**\n",
    "\n",
    "    Add more training examples; Increase dropout; Decrease features, etc.\n",
    "    \n",
    "\n",
    "6. **In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via  $ùëù(ùë§|ùë•)‚àùùëù(ùë•|ùë§)ùëù(ùë§)$ . How can you identify  ùëù(ùë§)  with regularization?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.7. Exercises\n",
    "\n",
    "1. **Try out what happens if you change the dropout probabilities for layers 1 and 2. In particular, what happens if you switch the ones for both layers?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "## network 1\n",
    "net467_1 = nn.Sequential()\n",
    "net467_1.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net467_1.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_1.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_1, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## network 2, switch dropout rate\n",
    "net467_2 = nn.Sequential()\n",
    "net467_2.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10))\n",
    "net467_2.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_2.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_2, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Increase the number of epochs and compare the results obtained when using dropout with those when not using it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## network 3\n",
    "num_epochs = 50\n",
    "net467_3 = nn.Sequential()\n",
    "net467_3.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10))\n",
    "net467_3.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_3.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_3, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compute the variance of the the activation random variables after applying dropout.**\n",
    "\n",
    "    Dropout replaces an activation  ‚Ñé  with a random variable  ‚Ñé‚Ä≤  with expected value  ‚Ñé  and with variance given by the dropout probability  ùëù .\n",
    "    \n",
    "    $$\\begin{split}\\begin{aligned}\n",
    "        h' =\n",
    "        \\begin{cases}\n",
    "            0 & \\text{ with probability } p \\\\\n",
    "            \\frac{h}{1-p} & \\text{ otherwise}\n",
    "        \\end{cases}\n",
    "        \\end{aligned}\\end{split}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Why should you typically not using dropout?**\n",
    "\n",
    "    Dropout can help with regularization, but at a risk of lossing improtant information. Especially applying Dropout in the first layer will lead to significant inforamtion loss.\n",
    "    \n",
    "\n",
    "5. **If changes are made to the model to make it more complex, such as adding hidden layer units, will the effect of using dropout to cope with overfitting be more obvious?**\n",
    "\n",
    "   ??????\n",
    "   \n",
    "   For an overfitted model, adding a hidden layer with dropout may not help. Especially in the situation that the effective neurons in this layer is larger than the number of neurons in the later layers, since this is equal to adding an extra hidden layer. \n",
    "   \n",
    "\n",
    "6. **Using the model in this section as an example, compare the effects of using dropout and weight decay. What if dropout and weight decay are used at the same time?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## random simulate dataset\n",
    "n_train, n_test, num_inputs = 20, 100, 200\n",
    "true_w, true_b = nd.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "\n",
    "features = nd.random.normal(shape=(n_train + n_test, num_inputs))\n",
    "labels = nd.dot(features, true_w) + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)\n",
    "train_features, test_features = features[:n_train, :], features[n_train:, :]\n",
    "train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(train_features, train_labels), batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_plot_gluon_467_6(dropout, wd, num_epochs=50, lr=0.01, batch_size=256):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Dense(10))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "\n",
    "    trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',\n",
    "                              {'learning_rate': lr, 'wd': wd})\n",
    "    # The bias parameter has not decayed. Bias names generally end with \"bias\"\n",
    "    trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',\n",
    "                              {'learning_rate': lr})\n",
    "    train_ls, test_ls = [], []\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            # Call the step function on each of the two Trainer instances to\n",
    "            # update the weight and bias separately\n",
    "            trainer_w.step(batch_size)\n",
    "            trainer_b.step(batch_size)\n",
    "        train_ls.append(loss(net(train_features),\n",
    "                             train_labels).mean().asscalar())\n",
    "        test_ls.append(loss(net(test_features),\n",
    "                            test_labels).mean().asscalar())\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n",
    "                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n",
    "#     print('L2 norm of w:', net[0].weight.data().norm().asscalar())\n",
    "\n",
    "fit_and_plot_gluon_467_6(dropout=0.5, wd=0) \n",
    "fit_and_plot_gluon_467_6(dropout=0.5, wd=3)\n",
    "fit_and_plot_gluon_467_6(dropout=0, wd=3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. **What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?**\n",
    "\n",
    "    The regularization effect will be the same. If we turn partial of the weights to be zero, these neurons won't learn any signal from the inputs, which will have the similar functionality as dropout on activation.\n",
    "\n",
    "\n",
    "8. **Replace the dropout activation with a random variable that takes on values of  $[0,ùõæ/2,ùõæ]$ . Can you design something that works better than the binary dropout function? Why might you want to use it? Why not?**\n",
    "\n",
    "    Define the following dropout activation function:\n",
    "\n",
    "    $$\\begin{split}\\begin{aligned}\n",
    "    h' =\n",
    "    \\begin{cases}\n",
    "        0 & \\text{ with probability } p_1 \\\\\n",
    "        ùõæ/2 * h & \\text{ with probability } (1 - p_1)  p_2 \\\\\n",
    "        ùõæ * h & \\text{ with probability } (1 - p_1)(1 - p_2)\n",
    "    \\end{cases}\n",
    "    \\end{aligned}\\end{split}$$\n",
    "\n",
    "    The has the expectation remained unchanged, we need to have\n",
    "    $$ 0 + (ùõæ/2) h (1 - p_1)  p_2 +  ùõæ  h  (1 - p_1)(1 - p_2)  = h$$\n",
    "    Thus,\n",
    "     $$ùõæ = \\frac{2}{(2-p_2)(1-p_1)}$$\n",
    "\n",
    "    For example, if we let $p_1=0.2, p_2=0.75$, then $ùõæ = 2$ by the above formula,\n",
    "    \n",
    "     $$\\begin{split}\\begin{aligned}\n",
    "    h' =\n",
    "    \\begin{cases}\n",
    "        0 & \\text{ with probability } 0.2 \\\\\n",
    "        h & \\text{ with probability } 0.6 \\\\\n",
    "        2 h & \\text{ with probability } 0.2\n",
    "    \\end{cases}\n",
    "    \\end{aligned}\\end{split}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.6. Exercises\n",
    "\n",
    "1. **Assume that the inputs  X  are matrices. What is the dimensionality of the gradients?**\n",
    "\n",
    "    Notice the dimensionality of each layer's graditents is equal the dimensionality of each layer's weighs. i.e. \n",
    "    $$ dim(\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}) = dim(\\mathbf{W}^{(1)})$$\n",
    "    \n",
    "    If the inputs $X \\in \\mathbb{R}^{d \\times c}$ are matrices at each row, the weight matrix $\\mathbf{W}^{(1)}$ would have a dimension equal to ${h \\times (d \\times  c)}$, where h is the hidden layer dimension.\n",
    "    \n",
    "\n",
    "2. **Add a bias to the hidden layer of the model described in this chapter.**\n",
    "    a. **Draw the corresponding compute graph.**\n",
    "    b. **Derive the forward and backward propagation equations.**\n",
    "    \n",
    "    b. Given a hidden layer with a bias:\n",
    "    $$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\\\\n",
    "      \\mathbf{h}= \\phi (\\mathbf{z}) \\\\\n",
    "      \\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)}\\\\\n",
    "      L = l(\\mathbf{o}, y) \\\\\n",
    "      s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) \\\\\n",
    "      J = L + s$$\n",
    "      \n",
    "      hence the backward propagation will be\n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}\n",
    "        = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$$\n",
    "        \n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}}\n",
    "        = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{b}^{(2)}}\\right)  + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{b}^{(2)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\times 1 + 0\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}}$$\n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}} + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} {\\mathbf{W}^{(2)}}^\\top \\odot \\phi'\\left(\\mathbf{z}\\right) {\\mathbf{x}}^\\top + \\lambda \\mathbf{W}^{(1)}$$\n",
    "        \n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}^{(1)}} + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{b}^{(1)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} {\\mathbf{W}^{(2)}}^\\top \\odot \\phi'\\left(\\mathbf{z}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Compute the memory footprint for training and inference in model described in the current chapter.**\n",
    "\n",
    "    Training: need memory for $$ \\frac{\\partial J}{\\partial \\mathbf{o}}, {\\mathbf{W}^{(2)}}, {\\mathbf{W}^{(1)}}, \\phi'\\left(\\mathbf{z}\\right), {\\mathbf{x}},$$\n",
    "    Inference: only need memory for $$\\mathbf{W}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(1)}, \\mathbf{b}^{(2)}$$\n",
    "\n",
    "\n",
    "4. **Assume that you want to compute second derivatives. What happens to the compute graph? Is this a good idea?**\n",
    "\n",
    "    \n",
    "\n",
    "5. **Assume that the compute graph is too large for your GPU.**\n",
    "\n",
    "    a.**Can you partition it over more than one GPU?**\n",
    "        \n",
    "        By default, MXNet uses data parallelism to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices.\n",
    "        \n",
    "    b.**What are the advantages and disadvantages over training on a smaller minibatch?**\n",
    "    \n",
    "    Advantages:\n",
    "        1. More robust convergence, avoiding local minima (as its model update frequency is higher than batch gradient descent.)\n",
    "        2. Computationally efficient process than stochastic gradient descent.\n",
    "        3. Memory efficient than batch gradient descent.\n",
    "        \n",
    "    Disadvantages:\n",
    "        1. Tune minibatch hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4. Exercises\n",
    "\n",
    "1. **Can you design other cases of symmetry breaking besides the permutation symmetry?**\n",
    "\n",
    "2. **Can we initialize all weight parameters in linear regression or in softmax regression to the same value?**\n",
    "\n",
    "\n",
    "3. **Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?**\n",
    "\n",
    "\n",
    "4. **If we know that some terms diverge, can we fix this after the fact? Look at the paper on LARS by You, Gitman and Ginsburg, 2017 for inspiration.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.5. Exercises\n",
    "\n",
    "1. **What could happen when we change the behavior of a search engine? What might the users do? What about the advertisers?**\n",
    "\n",
    "2. **Implement a covariate shift detector. Hint - build a classifier.**\n",
    "\n",
    "\n",
    "3. **Implement a covariate shift corrector.**\n",
    "\n",
    "\n",
    "4. **What could go wrong if training and test set are very different? What would happen to the sample weights?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5\n",
    "#### 5.1.6. Exercises\n",
    "\n",
    "1. **What kind of error message will you get when calling an __init__ method whose parent class not in the __init__ function of the parent class?**\n",
    "\n",
    "    InitializationError, i.e. cannot initialize the related parameters (the weights).\n",
    "    \n",
    "\n",
    "2. **What kinds of problems will occur if you remove the asscalar function in the FancyMLP class?**\n",
    "\n",
    "    Returns a scalar whose value is copied from the resulted array.\n",
    "    \n",
    "    \n",
    "3. **What kinds of problems will occur if you change self.net defined by the Sequential instance in the NestMLP class to self.net = [nn.Dense(64, activation='relu'), nn. Dense(32, activation='relu')]?**\n",
    "\n",
    "    If change *nn.Sequential()* to the above list, then we cannot add additional network to NestMLP, since the function of Sequential is the concatenations of layers and blocks. Following code will give you the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac890d95d2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestMLP_exercise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ac890d95d2b6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestMLP_exercise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# nn.Sequential()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         self.net.add(nn.Dense(64, activation='relu'),\n\u001b[0m\u001b[1;32m      6\u001b[0m                      nn.Dense(32, activation='relu'))\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "class NestMLP_exercise(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP_exercise, self).__init__(**kwargs)\n",
    "        self.net = [nn.Dense(64, activation='relu'), nn. Dense(32, activation='relu')]  # nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                     nn.Dense(32, activation='relu'))\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(self.net(x))\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(NestMLP_exercise(), nn.Dense(20))\n",
    "net.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Implement a block that takes two blocks as an argument, say net1 and net2 and returns the concatenated output of both networks in the forward pass (this is also called a parallel block).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00052053 -0.03974626]\n",
       " [-0.00133751 -0.01168625]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParallelMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ParallelMLP, self).__init__(**kwargs)\n",
    "        self.input_net1 = nn.Sequential()\n",
    "        for item1 in net1:\n",
    "            self.input_net1.add(item1)\n",
    "        self.input_net1.add(nn.Dense(1))\n",
    "        \n",
    "        self.input_net2 = nn.Sequential()\n",
    "        for item2 in net2:\n",
    "            self.input_net2.add(item2)\n",
    "        self.input_net2.add(nn.Dense(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.input_net1(x) ## shape of out1 : (batch_size, 1)\n",
    "        out2 = self.input_net2(x) ## shape of out1 : (batch_size, 1)\n",
    "        out = mx.nd.concat(out1, out2, dim=-1) ## shape of out : (batch_size, 2)\n",
    "        return out\n",
    "\n",
    "    \n",
    "x = nd.random.uniform(shape=(2, 20))\n",
    "net1 = [nn.Dense(64, activation='relu'), nn. Dense(32, activation='relu')]\n",
    "net2 = [nn.Dense(64, activation='relu')]\n",
    "parallel = ParallelMLP()\n",
    "parallel.initialize()\n",
    "parallel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Assume that you want to concatenate multiple instances of the same network. Implement a factory function that generates multiple instances of the same block and build a larger network from it.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0.]\n",
       " [0. 0. 0.]]\n",
       "<NDArray 2x3 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mx.nd.zeros(shape=(2,3))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[-0.00043408]\n",
       "  [ 0.00071604]]\n",
       "\n",
       " [[-0.00023887]\n",
       "  [ 0.00123192]]\n",
       "\n",
       " [[ 0.00160318]\n",
       "  [ 0.00164159]]]\n",
       "<NDArray 3x2x1 @cpu(0)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LargeNetwork(nn.Block):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(LargeNetwork, self).__init__(**kwargs)\n",
    "        self.input_net = nn.Sequential()\n",
    "        for item in net:\n",
    "            self.input_net.add(item)\n",
    "        self.input_net.add(nn.Dense(1))\n",
    "        self.large_net = {}\n",
    "        \n",
    "    def __init_large_net(self, length):\n",
    "        for i in range(length):\n",
    "            self.large_net[i] = self.input_net\n",
    "        \n",
    "    def forward(self, input_list):\n",
    "        '''\n",
    "        input_list is a list of input instance of same shape\n",
    "        '''\n",
    "        out = mx.nd.zeros(shape=(len(input_list), len(input_list[0]), 1)) ## 1 is the output shape for this input_net\n",
    "        \n",
    "        ## initial large net if it does not exist\n",
    "        if len(self.large_net.keys()) == 0:\n",
    "            self.__init_large_net(len(input_list))\n",
    "            \n",
    "        for j, instance in enumerate(input_list):\n",
    "            net = self.large_net[j]\n",
    "            out[j,:] = net(instance) ## shape of out1 : (batch_size, 1)\n",
    "        return mx.nd.concat(out, dim=-1)\n",
    "\n",
    "    \n",
    "x = nd.random.uniform(shape=(2, 20))\n",
    "y = nd.random.uniform(shape=(2, 20))\n",
    "z = nd.random.uniform(shape=(2, 20))\n",
    "\n",
    "net = [nn.Dense(64, activation='relu'), nn.Dense(10, activation='relu')]\n",
    "large_network = LargeNetwork(net)\n",
    "large_network.initialize()\n",
    "large_network([x,y,z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5. Exercises\n",
    "\n",
    "1. **Use the FancyMLP definition of the previous section and access the parameters of the various layers.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fancymlp0_ (\n",
      "  Constant fancymlp0_rand_weight (shape=(20, 20), dtype=<class 'numpy.float32'>)\n",
      "  Parameter dense76_weight (shape=(20, 20), dtype=float32)\n",
      "  Parameter dense76_bias (shape=(20,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        # Random weight parameters created with the get_constant are not\n",
    "        # iterated during training (i.e. constant parameters)\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', nd.random.uniform(shape=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        # Use the constant parameters created, as well as the relu and dot\n",
    "        # functions of NDArray\n",
    "        x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        x = self.dense(x)\n",
    "        # Here in Control flow, we need to call asscalar to return the scalar\n",
    "        # for comparison\n",
    "        while x.norm().asscalar() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().asscalar() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n",
    "    \n",
    "x = nd.random.uniform(shape=(2, 20))\n",
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Look at the MXNet documentation and explore different initializers.**\n",
    "    \n",
    "    Constant, Normal, Xavier, Orthogonal, MSRAPrelu, etc.\n",
    "    http://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.initializer.Mixed\n",
    "    \n",
    "\n",
    "3. **Try accessing the model parameters after net.initialize() and before net(x) to observe the shape of the model parameters. What changes? Why?**\n",
    "\n",
    "    The shape of layer changed. In the below example, for eg, \"dense77_weight\" was of shape=(20, 0) before net(x), but became shape=(20, 20) after input x. Since dim==-1 of x is 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20) \n",
      "\n",
      "fancymlp4_ (\n",
      "  Constant fancymlp4_rand_weight (shape=(20, 20), dtype=<class 'numpy.float32'>)\n",
      "  Parameter dense80_weight (shape=(20, 0), dtype=float32)\n",
      "  Parameter dense80_bias (shape=(20,), dtype=float32)\n",
      ")\n",
      "fancymlp4_ (\n",
      "  Constant fancymlp4_rand_weight (shape=(20, 20), dtype=<class 'numpy.float32'>)\n",
      "  Parameter dense80_weight (shape=(20, 20), dtype=float32)\n",
      "  Parameter dense80_bias (shape=(20,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, \"\\n\")\n",
    "net3 = FancyMLP()\n",
    "net3.initialize()\n",
    "print(net3.collect_params())\n",
    "net3(x)\n",
    "print(net3.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Construct a multilayer perceptron containing a shared parameter layer and train it. During the training process, observe the model parameters and gradients of each layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential50_ (\n",
      "  Parameter dense139_weight (shape=(8, 20), dtype=float32)\n",
      "  Parameter dense139_bias (shape=(8,), dtype=float32)\n",
      "  Parameter dense137_weight (shape=(8, 8), dtype=float32)\n",
      "  Parameter dense137_bias (shape=(8,), dtype=float32)\n",
      "  Parameter dense140_weight (shape=(1, 8), dtype=float32)\n",
      "  Parameter dense140_bias (shape=(1,), dtype=float32)\n",
      ")\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 8x8 @cpu(0)>\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 8x8 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "## ??????\n",
    "x = nd.random.uniform(shape=(2, 20))\n",
    "y = nd.random.uniform(shape=(2, 1))\n",
    "\n",
    "net4 = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "shared_reuse = nn.Dense(8, activation='relu', params=shared.params)\n",
    "net4.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        shared_reuse,\n",
    "        nn.Dense(1))\n",
    "net4.initialize()\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "with mx.autograd.record():\n",
    "    y_hat = net4(x)\n",
    "    print(net4.collect_params())\n",
    "    l = loss(y_hat, y).sum()\n",
    "l.backward()\n",
    "print(shared.weight.grad())\n",
    "print(shared_reuse.weight.grad())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Why is sharing parameters a good idea?**\n",
    "\n",
    "    Sharing parameters can save memory in general and have specific benefits for the following:\n",
    "    \n",
    "    For CNN in image recognition, sharing parameters gives the network the ability to look for a given feature everywhere in the image, rather than in just a certain area. \n",
    "    \n",
    "    For RNN, it shares parameters across time steps of the sequence, so it can generalize well to examples of different sequence length.\n",
    "    \n",
    "    For autoencoder, encoder and decoder share parameters. In a single layer autoencoder with linear activation, sharing weights forces orthogonality among different hidden layer of weight matrix.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5. Exercises\n",
    "\n",
    "1. **What happens if you specify only parts of the input dimensions. Do you still get immediate initialization?**\n",
    "\n",
    "    No. Initialization will occur right before the forward pass.\n",
    "    \n",
    "\n",
    "2. **What happens if you specify mismatching dimensions?**\n",
    "\n",
    "    Error like the following will occur:\n",
    "    \n",
    "    ```\n",
    "    Check failed: from.shape() == to->shape() operands shape mismatchfrom.shape = (126,) to.shape=(12,)\n",
    "    ```\n",
    "    \n",
    "3. **What would you need to do if you have input of varying dimensionality? Hint - look at parameter tying.**\n",
    "\n",
    "    See below printed results for details. Notice that the seiral number xxx of the **shared :  sequentialxxx** is always the same, while that of **unique :  sequentialxxx** is different for different dimensionality inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "^^^^^^^^^^^^^^^^^^^^^ features2 ^^^^^^^^^^^^^^^^^^^^^\n",
      "****************** epoch 0 ******************\n",
      "unique :  sequential117_ (\n",
      "  Parameter dense261_weight (shape=(16, 3), dtype=float32)\n",
      "  Parameter dense261_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense262_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense262_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "shared :  sequential118_ (\n",
      "  Parameter dense263_weight (shape=(16, 8), dtype=float32)\n",
      "  Parameter dense263_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense264_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense264_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "****************** epoch 1 ******************\n",
      "unique :  sequential117_ (\n",
      "  Parameter dense261_weight (shape=(16, 3), dtype=float32)\n",
      "  Parameter dense261_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense262_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense262_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "shared :  sequential118_ (\n",
      "  Parameter dense263_weight (shape=(16, 8), dtype=float32)\n",
      "  Parameter dense263_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense264_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense264_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "\n",
      "^^^^^^^^^^^^^^^^^^^^^ features2 ^^^^^^^^^^^^^^^^^^^^^\n",
      "****************** epoch 0 ******************\n",
      "unique :  sequential119_ (\n",
      "  Parameter dense265_weight (shape=(16, 6), dtype=float32)\n",
      "  Parameter dense265_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense266_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense266_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "shared :  sequential118_ (\n",
      "  Parameter dense263_weight (shape=(16, 8), dtype=float32)\n",
      "  Parameter dense263_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense264_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense264_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "****************** epoch 1 ******************\n",
      "unique :  sequential119_ (\n",
      "  Parameter dense265_weight (shape=(16, 6), dtype=float32)\n",
      "  Parameter dense265_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense266_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense266_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "shared :  sequential118_ (\n",
      "  Parameter dense263_weight (shape=(16, 8), dtype=float32)\n",
      "  Parameter dense263_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense264_weight (shape=(8, 16), dtype=float32)\n",
      "  Parameter dense264_bias (shape=(8,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def get_net535_init():\n",
    "    net535 = nn.Sequential()\n",
    "    net535.add(nn.Dense(16, activation='relu'), nn.Dense(8))\n",
    "    net535.initialize()  ## reinit one specific layer\n",
    "    return(net535)\n",
    "\n",
    "def train_535(net535_unique, net535_shared, X, y, test_iter=None, loss=gloss.L2Loss(), \n",
    "              num_epochs=2, batch_size=32, params=None, lr=0.05):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"****************** epoch {} ******************\".format(epoch))\n",
    "        with autograd.record():\n",
    "            try:\n",
    "                mid = net535_unique(X)\n",
    "                print(\"unique : \", net535_unique.collect_params())\n",
    "                \n",
    "            except:\n",
    "                ## if existing network shape does not match, reinitialize a network\n",
    "                net535_unique_new = get_net535_init()\n",
    "                net535_unique = net535_unique_new\n",
    "                mid = net535_unique(X)\n",
    "                print(\"unique : \", net535_unique.collect_params())\n",
    "                \n",
    "            ## the shared network does not need to be reinit as the shape is consistant\n",
    "            y_hat = net535_shared(mid)\n",
    "            print(\"shared : \", net535_shared.collect_params())\n",
    "\n",
    "\n",
    "## Generate dataset\n",
    "batch_size = 10\n",
    "features1 = nd.random.uniform(shape=(batch_size, 3))\n",
    "print(features1.shape)\n",
    "features2 = nd.random.uniform(shape=(batch_size, 6))\n",
    "print(features2.shape)\n",
    "\n",
    "y = nd.random.uniform(shape=(batch_size, 1))\n",
    "net535_unique_ex = get_net535_init()\n",
    "net535_shared_ex = get_net535_init()\n",
    "print(\"\\n^^^^^^^^^^^^^^^^^^^^^ features2 ^^^^^^^^^^^^^^^^^^^^^\".format(features))\n",
    "train_535(net535_unique_ex, net535_shared_ex, features1, y)\n",
    "print(\"\\n^^^^^^^^^^^^^^^^^^^^^ features2 ^^^^^^^^^^^^^^^^^^^^^\".format(features))\n",
    "train_535(net535_unique_ex, net535_shared_ex, features2, y)\n",
    "\n",
    "# train_535(net=net535, train_iter=data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4. Exercises\n",
    "\n",
    "1. **Design a layer that learns an affine transform of the data, i.e. it removes the mean and learns an additive parameter instead.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-2. -1.  0.  1.  2.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(nd.array([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Design a layer that takes an input and computes a tensor reduction, i.e. it returns  $y_k = \\sum_{i,j} W_{ijk} x_i x_j$.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.01547093 -0.00399414 -0.01535948]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TensorReductionLayer(nn.Block):\n",
    "    def __init__(self, k, x_shape, **kwargs):\n",
    "        super(TensorReductionLayer, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(x_shape, k, x_shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mid = nd.dot(self.weight.data(), x)\n",
    "        print(mid.shape)\n",
    "        out = nd.dot(x.T, mid)\n",
    "        return out.reshape(k)\n",
    "\n",
    "## sample random x with given length\n",
    "x_length = 5\n",
    "x = nd.random.uniform(shape=(x_length, 1))\n",
    "\n",
    "k = 3  ## k can be any integer\n",
    "TRlayer = TensorReductionLayer(k, x_length)\n",
    "TRlayer.initialize()\n",
    "TRlayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Design a layer that returns the leading half of the Fourier coefficients of the data. Hint - look up the fft function in MXNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "class FourierLayer(nn.Block):\n",
    "    def __init__(self, k, x_shape, **kwargs):\n",
    "        super(FourierLayer, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(x_shape, k, x_shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mid = nd.dot(self.weight.data(), x)\n",
    "        print(mid.shape)\n",
    "        out = nd.dot(x.T, mid)\n",
    "        return out.reshape(k)\n",
    "\n",
    "## sample random x with given shape\n",
    "data = np.random.normal(0,1,(3,4))\n",
    "out = mx.contrib.ndarray.fft(data = mx.nd.array(data,ctx = mx.gpu(0)))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.4. Exercises\n",
    "\n",
    "1. **Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?**\n",
    "\n",
    "    a. Saving intermediate results (checkpointing) to ensure that we don‚Äôt lose several days worth of computation when running a long training process.\n",
    "    \n",
    "    b. To load a pretrained model for fine tuning.\n",
    "    \n",
    "\n",
    "2. **Assume that we want to reuse only parts of a network to be incorporated into a network of a different architecture. How would you go about using, say the first two layers from a previous network in a new network.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet6_ (\n",
      "  Parameter alexnet6_conv0_weight (shape=(64, 3, 11, 11), dtype=<class 'numpy.float32'>)\n",
      "  Parameter alexnet6_conv0_bias (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "alexnet6_ (\n",
      "  Parameter alexnet6_conv1_weight (shape=(192, 64, 5, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter alexnet6_conv1_bias (shape=(192,), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import model_zoo\n",
    "alexnet = model_zoo.vision.alexnet(pretrained=True)\n",
    "print(alexnet.collect_params('alexnet.*_conv0.*'))\n",
    "print(alexnet.collect_params('alexnet.*_conv1.*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **How would you go about saving network architecture and parameters? What restrictions would you impose on the architecture?**\n",
    "\n",
    "    In order to reload a trained model, we need to generate the architecture in code and then load the parameters from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        self.output = nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))\n",
    "\n",
    "## train a network and save\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "x = nd.random.uniform(shape=(2, 20))\n",
    "y = net(x)\n",
    "clone.save_parameters('mlp.params')\n",
    "\n",
    "## define the architecture and then reload parameters\n",
    "clone = MLP()\n",
    "clone.load_parameters('mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.5. Exercises\n",
    "\n",
    "1. **Try a larger computation task, such as the multiplication of large matrices, and see the difference in speed between the CPU and GPU. What about a task with a small amount of calculations?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "s = 4096\n",
    "\n",
    "A = nd.random.normal(shape=(s, s))\n",
    "B = nd.random.normal(shape=(s, s))\n",
    "tic = time.time()\n",
    "C = nd.dot(A, B)\n",
    "C.wait_to_read()\n",
    "print(\"On CPU : Matrix by matrix: \" + str(time.time() - tic) + \" seconds\")\n",
    "\n",
    "\n",
    "A1 = A.copyto(mx.gpu(1))\n",
    "B1 = B.copyto(mx.gpu(1))\n",
    "tic = time.time()\n",
    "C = nd.dot(A, B)\n",
    "C.wait_to_read()\n",
    "print(\"On GPU : Matrix by matrix: \" + str(time.time() - tic) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **How should we read and write model parameters on the GPU?**\n",
    "\n",
    "    Use `net.load_parameters(file_name, ctx=ctx)` to read model parameters, and `net.save_parameters(file_name)` to save model parameters.\n",
    "    \n",
    "\n",
    "3. **Measure the time it takes to compute 1000 matrix-matrix multiplications of  100√ó100 matrices and log the matrix norm  $tr(MM^‚ä§)$  one result at a time vs. keeping a log on the GPU and transferring only the final result.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "for j in range(1000):\n",
    "    M = nd.random.normal(shape=(100, 100))\n",
    "    C = nd.sum(nd.diag(nd.dot(M, M.T)))\n",
    "    C.wait_to_read()\n",
    "print(\"Read one-by-one on GPU : \" + str(time.time() - tic) + \" seconds\")\n",
    "\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "D = nd.zeros(shape=(1000,))\n",
    "for j in range(1000):\n",
    "    M = nd.random.normal(shape=(100, 100))\n",
    "    D[i] = nd.sum(nd.diag(nd.dot(M, M.T)))\n",
    "    \n",
    "D.wait_to_read()\n",
    "print(\"Read all at once on GPU : \" + str(time.time() - tic) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Measure how much time it takes to perform two matrix-matrix multiplications on two GPUs at the same time vs. in sequence on one GPU (hint - you should see almost linear scaling).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 4096)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4096, 4096)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two matrix-matrix multiplications in sequence on GPU : 0.009020805358886719 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 4096, 4096)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two matrix-matrix multiplications at the same time on GPU : 0.004949331283569336 seconds\n"
     ]
    }
   ],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "s = 4096\n",
    "tic = time.time()\n",
    "for j in range(2):\n",
    "    M = nd.random.normal(shape=(s, s))\n",
    "    C = nd.dot(M, M.T)\n",
    "    C.shape\n",
    "#     C.wait_to_read()\n",
    "print(\"Two matrix-matrix multiplications in sequence on GPU : \" + str(time.time() - tic) + \" seconds\")\n",
    "\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "M = nd.random.normal(shape=(2, s, s))\n",
    "N = nd.random.normal(shape=(s, s))\n",
    "D = nd.dot(M, N)\n",
    "D.shape\n",
    "print(\"Two matrix-matrix multiplications at the same time on GPU : \" + str(time.time() - tic) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6\n",
    "#### 6.1 Exercises\n",
    "\n",
    "1. **Assume that the size of the convolution mask is ‚àÜ = 0. Show that in this case the convolutional mask implements an MLP independently for each set of channels.**\n",
    "\n",
    "    For any tensor k, since $‚àÜ = 0$, the learner are independent. i.e.\n",
    "    $$h[i,j,k] = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} V[a,b,k] \\cdot x[i+a,j+b]\n",
    "               = V[0,0,k] \\cdot x[i,j]$$\n",
    "               \n",
    "\n",
    "2. **Why might translation invariance not be a good idea after all? Does it make sense for pigs to fly?**\n",
    "\n",
    "    In translation invariance, we assume that we would recognize an object wherever it is in an image. It is only reasonable to assume that the location of the object shouldn‚Äôt matter too much to determine whether the object is there. For example, a face is still a face regardless of whether it is moved horizontally or vertically in an image.**\n",
    "\n",
    "\n",
    "\n",
    "3. **What happens at the boundary of an image?** \n",
    "\n",
    "    On the boundaries we encounter the problem that we keep on losing pixels. (Without padding)\n",
    "    \n",
    "    \n",
    "4. **Derive an analogous convolutional layer for audio.**\n",
    "\n",
    "    Mel spectrogram transform the input raw sequence to a 2D feature map where one dimension represents time and the other one represents frequency and the values represents amplitude. https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
    "\n",
    "    Moving a sound event horizontally offsets its position in time and it can be argued that a sound event means the same thing regardless of when it happens. However, moving a sound vertically might influence its meaning:¬†Moving the frequencies of a male voice upwards could change its meaning from man to child or goblin, for example.\n",
    "\thttps://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd\n",
    "\n",
    "\n",
    "5. **What goes wrong when you apply the above reasoning to text? Hint - what is the structure of language?**\n",
    "        \n",
    "    Language is a sequence data, hence we cannot assume a translation invariance, i.e. the location of each word matters. Sequence modeling is more effective. \n",
    "        \n",
    "        \n",
    "6. **Prove that f‚äõg=g‚äõf.**\n",
    "\n",
    "    Let $x-z = y$, then\n",
    "    $$[f \\circledast g](x) = \\int_{\\mathbb{R}^d} f(z) g(x-z) dz \n",
    "                            = \\int_{\\mathbb{R}^d} f(x-y) g(y) dy\n",
    "                            = [g \\circledast f](x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Exercises\n",
    "\n",
    "1. **Construct an image X with diagonal edges.**\n",
    "\n",
    "    ‚Ä¢ **What happens if you apply the kernel K to it?**\n",
    "    \n",
    "    ‚Ä¢ **What happens if you transpose X?**\n",
    "    \n",
    "    ‚Ä¢ **What happens if you transpose K?**\n",
    "\n",
    "\n",
    "2. **When you try to automatically find the gradient for the Conv2D class we created, what kind of error message do you see?**\n",
    "    ??????\n",
    "    \n",
    "    \n",
    "3. **How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel arrays?**\n",
    "    In the two-dimensional cross-correlation operation, the convolution window starts from the top-left of the input array, and slides in the input array from left to right and top to bottom. [See details in 6.2.1]\n",
    "\n",
    "\n",
    "4. **Design some kernels manually.**\n",
    "    ‚Ä¢**What is the form of a kernel for the second derivative?**\n",
    "    \n",
    "    ‚Ä¢ **What is the kernel for the Laplace operator?**\n",
    "    https://math.stackexchange.com/questions/483585/kernels-to-compute-second-order-derivative-of-digital-image\n",
    "        \n",
    "\t‚Ä¢ **What is the kernel for an integral?**\n",
    "    http://mathworld.wolfram.com/IntegralKernel.html\n",
    "        \n",
    "\t‚Ä¢ **What is the minimum size of a kernel to obtain a derivative of degree d?**\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Exercises\n",
    "\n",
    "1. **For the last example in this section, use the shape calculation formula to calculate the output shape to see if it is consistent with the experimental results.**\n",
    "\n",
    "\n",
    "2. **Try other padding and stride combinations on the experiments in this section.**\n",
    "\n",
    "\n",
    "3. **For audio signals, what does a stride of 2 correspond to?**\n",
    "\n",
    "    In the time dimension, stride of 2 aggregates every 2 timestamps. \n",
    "    \n",
    "    In the frequency dimension, ‚Ä¶\n",
    "    \n",
    "4. **What are the computational benefits of a stride larger than 1.**\n",
    "    \n",
    "    Save memory and reduce computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Exercises\n",
    "\n",
    "1. **Assume that we have two convolutional kernels of size k1 and k2 respectively (with no nonlinearity in between).** \n",
    "\n",
    "    ‚Ä¢ **Prove that the result of the operation can be expressed by a single convolution.** \n",
    "    \n",
    "    Convolution associate law.\n",
    "    \n",
    "    ‚Ä¢ **What is the dimensionality of the equivalent single convolution?**\n",
    "    \n",
    "    k1+k2-1\n",
    "    \n",
    "    ‚Ä¢ **Is the converse true?**\n",
    "    \n",
    "    \n",
    "2. **Assume an input shape of ci √óh√ów and a convolution kernel with the shape co √óci √ókh √ókw, padding of (ph,pw), and stride of (sh,sw).**\n",
    "\n",
    "    ‚Ä¢ **What is the computational cost (multiplications and additions) for the forward computation?** \n",
    "    \n",
    "    ‚Ä¢ **What is the memory footprint?** \n",
    "    \n",
    "    $O(c_i c_o k_h k_w m_h m_w)$\n",
    "    \n",
    "    ‚Ä¢ **What is the memory footprint for the backward computation?**\n",
    "    \n",
    "    ‚Ä¢ **What is the computational cost for the backward computation?**\n",
    "    \n",
    "\thttps://kasperfred.com/posts/computational-complexity-of-neural-networks\n",
    "\n",
    "\n",
    "3. **By what factor does the number of calculations increase if we double the number of input channels ci and the number of output channels co? What happens if we double the padding?**\n",
    "\n",
    "\n",
    "4. **If the height and width of the convolution kernel is $k_h = k_w = 1$, what is the complexity of the forward computation?**\n",
    "\n",
    "    $O(c_i*c_o*h*w)$\n",
    "\n",
    "\n",
    "5. **Are the variables Y1 and Y2 in the last example of this section exactly the same? Why?**\n",
    "\n",
    "    Yes. The main computation of the 1 √ó 1 convolution occurs on the channel dimension. And $k_h & k_w == 1$ in both function.\n",
    "    \n",
    "    \n",
    "6. **How would you implement convolutions using matrix multiplication when the convolution window is not 1√ó1 ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Exercises\n",
    "\n",
    "1. **Implement average pooling as a convolution.**\n",
    "\n",
    "    6.5.1\n",
    "    \n",
    "    \n",
    "2. **What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size  c√óh√ów , the pooling window has a shape of  ph√ópw  with a padding of  $(p_h,p_w)$  and a stride of  $(s_h,s_w)$.**\n",
    "\n",
    "    $c√ó[(I_h - p_h + pad_h + s_h)/s_h]√ó[(l_w - p_w + pad_w + s_w)/s_w]$\n",
    "    \n",
    "\n",
    "3. **Why do you expect maximum pooling and average pooling to work differently?**\n",
    "\n",
    "    ‚Ä¢ Max pooling: the strongest pattern signal in a window\n",
    "    \n",
    "    ‚Ä¢ Average pooling:  The average signal strength in a window \n",
    "\n",
    "    ![title](image/textbook_solution_6.5.3.png)\n",
    "    \n",
    "    \n",
    "4. **Do we need a separate minimum pooling layer? Can you replace it with another operation?**\n",
    "\n",
    "    $argmin(Xa_{ij}) = argmax( -1 * Xa_{ij})$, hence min-pooling can be modeling through CNN and max-pooling\n",
    "    \n",
    "    \n",
    "5. **Is there another operation between average and maximum pooling that you could consider (hint - recall the softmax)? Why might it not be so popular?**\n",
    "    \n",
    "    A pooling layer is to alleviate the excessive sensitivity of the convolutional layer to location., i.e. reduce the resolution.\n",
    "    Softmax computation cost is too high.\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Exercises\n",
    "\n",
    "1. **Replace the average pooling with max pooling. What happens?**\n",
    "\n",
    "\n",
    "2. **Try to construct a more complex network based on LeNet to improve its accuracy.**\n",
    "Adjust the convolution window size.\n",
    "Adjust the number of output channels.\n",
    "Adjust the activation function (ReLU?).\n",
    "Adjust the number of convolution layers.\n",
    "Adjust the number of fully connected layers.\n",
    "Adjust the learning rates and other training details (initialization, epochs, etc.)\n",
    "Try out the improved network on the original MNIST dataset.\n",
    "\n",
    "\n",
    "3. **Display the activations of the first and second layer of LeNet for different inputs (e.g. sweaters, coats).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7 Exercises\n",
    "\n",
    "1. **Try increasing the number of epochs. Compared with LeNet, how are the results different? Why?**\n",
    "\n",
    "\n",
    "2. **AlexNet may be too complex for the Fashion-MNIST data set.**\n",
    "\n",
    "    a. **Try to simplify the model to make the training faster, while ensuring that the accuracy does not drop significantly.**\n",
    "    \n",
    "    b. **Can you design a better model that works directly on  28√ó28  images.**\n",
    "    \n",
    "    \n",
    "3. **Modify the batch size, and observe the changes in accuracy and GPU memory.**\n",
    "\n",
    "\n",
    "4. **Rooflines**\n",
    "\n",
    "    ![title](image/textbook_solution_6.7.4.png)\n",
    "    \n",
    "    a. **What is the dominant part for the memory footprint of AlexNet?**\n",
    "    \n",
    "    Dense1 - 26 millions of parameters\n",
    "    \n",
    "    b. **What is the dominant part for computation in AlexNet?**\n",
    "    \n",
    "    Conv2 - 16 millions of FLOP\n",
    "    \n",
    "    c. **How about memory bandwidth when computing the results?**\n",
    "    \n",
    "    \n",
    "5. **Apply dropout and ReLU to LeNet5. Does it improve? How about preprocessing?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.8 Exercises\n",
    "\n",
    "1. **When printing out the dimensions of the layers we only saw 8 results rather than 11. Where did the remaining 3 layer informations go?**\n",
    "\n",
    "    There are 3 pairs of CNN (3 vgg blocks) has the same shape\n",
    "    \n",
    "2.  **Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. Try to analyze the reasons for this.**\n",
    "\n",
    "\n",
    "3. **Try to change the height and width of the images in Fashion-MNIST from 224 to 96. What influence does this have on the experiments?**\n",
    "\n",
    "\n",
    "4. **Refer to Table 1 in the original VGG Paper to construct other common models, such as VGG-16 or VGG-19.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.9 Exercises\n",
    "\n",
    "1. **Tune the hyper-parameters to improve the classification accuracy.**\n",
    "\n",
    "\n",
    "2. **Why are there two  1√ó1  convolutional layers in the NiN block? Remove one of them, and then observe and analyze the experimental phenomena.**\n",
    "\n",
    "\n",
    "3. **Calculate the resource usage for NiN:**\n",
    "    \n",
    "    ![title](image/textbook_solution_6.9.3.png)\n",
    "    \n",
    "    a. **What is the number of parameters?**\n",
    "    \n",
    "    b. **What is the amount of computation?**\n",
    "    \n",
    "    c. **What is the amount of memory needed during training?**\n",
    "    \n",
    "    d. **What is the amount of memory needed during inference?**\n",
    "\n",
    "4. **What are possible problems with reducing the  384√ó5√ó5  representation to a  10√ó5√ó5 representation in one step?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 7\n",
    "#### 7.1.5. Exercises\n",
    "\n",
    "\n",
    "1. **Improve the above model.**\n",
    "\n",
    "    a. **Incorporate more than the past 4 observations? How many do you really need?**\n",
    "    \n",
    "    b. **How many would you need if there were no noise? Hint - you can write  sin  and  cos  as a differential equation.**\n",
    "    \n",
    "    \n",
    "    c.**Can you incorporate older features while keeping the total number of features constant? Does this improve accuracy? Why?**\n",
    "    \n",
    "    \n",
    "    d.**Change the architecture and see what happens.**\n",
    "    \n",
    "    \n",
    "2. **An investor wants to find a good security to buy. She looks at past returns to decide which one is likely to do well. What could possibly go wrong with this strategy?**\n",
    "\n",
    "\n",
    "\n",
    "3. **Does causality also apply to text? To which extent?**\n",
    "\n",
    "\n",
    "\n",
    "4. **Give an example for when a latent variable autoregressive model might be needed to capture the dynamic of the data.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.5. Exercises\n",
    "1. **Suppose there are 100,000 words in the training data set. How many word frequencies and multi-word adjacent frequencies does a four-gram need to store?**\n",
    "\n",
    "    In English, the Zipf‚Äôs law in the n-gram data exhibits two regimes: one among words with frequencies above about 0.01% (Zipf‚Äôs exponent Œ≥ ‚âà 1) and another (Œ≥ ‚âà 1.4) among words with frequency below 0.0001%.\n",
    "\n",
    "    By Zipf's law, the normalized frequency of elements of rank k, $f(k;s,N)$ is:\n",
    "\n",
    "    $$ f(k;s;N) = \\frac{1}{k^s \\sum_1^N \\frac{1}{n^s}},$$ \n",
    "    where N = 100,000 is the number of words in the English language, s = 1.07 for unigram. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Review the smoothed probability estimates. Why are they not accurate? Hint - we are dealing with a contiguous sequence rather than singletons.**\n",
    "\n",
    "    Laplace smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Lots of contiguous sequence may not occurs in the bag of word, hence the smoothed probability is not accurate in the right tail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **How would you model a dialogue?**\n",
    "\n",
    "    First, we can decide which type of dialogue (conversational models) are we going to model (rule-based, retrieval-based, neural generative models, grounded/visual, chit-chat vs. task-based, etc.)?\n",
    "\n",
    "    Next, choose framework for modelling, such as for neural generative models choosed Semantically Conditioned LSTM-based model. (https://arxiv.org/abs/1508.01745); or Deep Reinforcement Learning for Dialogue Generation (https://aclweb.org/anthology/D16-1127) and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Estimate the exponent of Zipf‚Äôs law for unigrams, bigrams and trigrams.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.5. Exercises\n",
    "\n",
    "1. **If we use an RNN to predict the next character in a text sequence, how many output dimensions do we need?**\n",
    "\n",
    "    The output dimension should be the length of the dictionary of unique characters from both train and test dataset.\n",
    "\n",
    "\n",
    "2. **Can you design a mapping for which an RNN with hidden states is exact? Hint - what about a finite number of words?**\n",
    "\n",
    "\n",
    "3. **What happens to the gradient if you backpropagate through a long sequence?**\n",
    "\n",
    "    High powers of matrices can lead to explode or vanish gradients.\n",
    "\n",
    "\n",
    "4. **What are some of the problems associated with the simple sequence model described above?**\n",
    "\n",
    " a. numerically unstable;\n",
    " b. difficulty of long-term information preservation and short-term input skipping in latent variable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4.5. Exercises**\n",
    "\n",
    "1. **What other mini-batch data sampling methods can you think of?**\n",
    "\n",
    "    Non-uniform mini-batch sampling. (i.e. suppressing the probability of similar data points in the same mini-batch, which will reduce the stochastic gradient noise, leading to faster convergence).\n",
    "\n",
    "\n",
    "2. **Why is it a good idea to have a random offset?**\n",
    "    \n",
    "    In this way, we can get both coverage(by sequential partitioning strategies) and randomness.\n",
    "\n",
    "      a. **Does it really lead to a perfectly uniform distribution over the sequences on the document?**\n",
    "\n",
    "        Picking just a random set of initial positions is no good either since it does not guarantee uniform coverage of the array. For instance, if we pick  n  elements at random out of a set of  n  with random replacement, the probability for a particular element not being picked is  \n",
    "            \n",
    "                $$(1‚àí1/n)^n ‚Üí e^{‚àí1}$$\n",
    "\n",
    "      b. **What would you have to do to make things even more uniform?**\n",
    "\n",
    "            # Offset for the iterator over the data for uniform starts\n",
    "            offset = int(random.uniform(0,num_steps))\n",
    "            \n",
    "            \n",
    "5. **If we want a sequence example to be a complete sentence, what kinds of problems does this introduce in mini-batch sampling? Why would we want to do this anyway?**\n",
    "\n",
    "    Since a complete sentence is long, it is acceptable to discard half-empty mini-batch. Since these sequences are covered by part of other batches in mini-batch sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.8. Exercises**\n",
    "\n",
    "1. **Show that one-hot encoding is equivalent to picking a different embedding for each object.**\n",
    "    \n",
    "    Elementary row and column operations on a matrix are rank-preserving.\n",
    "\n",
    "2. **Adjust the hyperparameters to improve the perplexity.**\n",
    "    \n",
    "    a. **How low can you go? Adjust embeddings, hidden units, learning rate, etc.**\n",
    "    \n",
    "        \n",
    "    \n",
    "    b. **How well will it work on other books by H. G. Wells, e.g. The War of the Worlds.**\n",
    "    \n",
    "    \n",
    "3. **Run the code in this section without clipping the gradient. What happens?**\n",
    "\n",
    "\n",
    "4. **Set the pred_period variable to 1 to observe how the under-trained model (high perplexity) writes lyrics. What can you learn from this?**\n",
    "\n",
    "\n",
    "5. **Change adjacent sampling so that it does not separate hidden states from the computational graph. Does the running time change? How about the accuracy?**\n",
    "\n",
    "\n",
    "6. **Replace the activation function used in this section with ReLU and repeat the experiments in this section.**\n",
    "\n",
    "\n",
    "\n",
    "7. **Prove that the perplexity is the inverse of the harmonic mean of the conditional word probabilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
