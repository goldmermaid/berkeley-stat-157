{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1.6**\n",
    "\n",
    "1. **Assume that the size of the convolution mask is ∆ = 0. Show that in this case the convolutional mask implements an MLP independently for each set of channels.**\n",
    "\n",
    "\n",
    "2. **Why might translation invariance not be a good idea after all? Does it make sense for pigs to fly?**\n",
    "\n",
    "    In translation invariance, we assume that we would recognize an object wherever it is in an image. It is only reasonable to assume that the location of the object shouldn’t matter too much to determine whether the object is there. For example, a face is still a face regardless of whether it is moved horizontally or vertically in an image.**\n",
    "\n",
    "\n",
    "\n",
    "3. **What happens at the boundary of an image?** \n",
    "\n",
    "    On the boundaries we encounter the problem that we keep on losing pixels. (Without padding)\n",
    "    \n",
    "4. **Derive an analogous convolutional layer for audio.**\n",
    "\n",
    "    Mel spectrogram transform the input raw sequence to a 2D feature map where one dimension represents time and the other one represents frequency and the values represents amplitude. https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
    "\n",
    "    Moving a sound event horizontally offsets its position in time and it can be argued that a sound event means the same thing regardless of when it happens. However, moving a sound vertically might influence its meaning: Moving the frequencies of a male voice upwards could change its meaning from man to child or goblin, for example.\n",
    "\thttps://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd\n",
    "\n",
    "5. **What goes wrong when you apply the above reasoning to text? Hint - what is the structure of language?**\n",
    "        \n",
    "    Language is a sequence data, hence we cannot assume a translation invariance, i.e. the location of each word matters. Sequence modeling is more effective. \n",
    "        \n",
    "        \n",
    "6. **Prove that f⊛g=g⊛f.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2.7**\n",
    "\n",
    "1. **Construct an image X with diagonal edges.**\n",
    "\n",
    "    • **What happens if you apply the kernel K to it?**\n",
    "    \n",
    "    • **What happens if you transpose X?**\n",
    "    \n",
    "    • **What happens if you transpose K?**\n",
    "\n",
    "\n",
    "2. **When you try to automatically find the gradient for the Conv2D class we created, what kind of error message do you see?**\n",
    "    ??????\n",
    "    \n",
    "    \n",
    "3. **How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel arrays?**\n",
    "    In the two-dimensional cross-correlation operation, the convolution window starts from the top-left of the input array, and slides in the input array from left to right and top to bottom. [See details in 6.2.1]\n",
    "\n",
    "\n",
    "4. **Design some kernels manually.**\n",
    "    •**What is the form of a kernel for the second derivative?**\n",
    "    \n",
    "    • **What is the kernel for the Laplace operator?**\n",
    "    https://math.stackexchange.com/questions/483585/kernels-to-compute-second-order-derivative-of-digital-image\n",
    "        \n",
    "\t• **What is the kernel for an integral?**\n",
    "    http://mathworld.wolfram.com/IntegralKernel.html\n",
    "        \n",
    "\t• **What is the minimum size of a kernel to obtain a derivative of degree d?**\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3.4**\n",
    "\n",
    "1. **For the last example in this section, use the shape calculation formula to calculate the output shape to see if it is consistent with the experimental results.**\n",
    "\n",
    "\n",
    "2. **Try other padding and stride combinations on the experiments in this section.**\n",
    "\n",
    "\n",
    "3. **For audio signals, what does a stride of 2 correspond to?**\n",
    "\n",
    "    In the time dimension, stride of 2 aggregates every 2 timestamps. \n",
    "    \n",
    "    In the frequency dimension, …\n",
    "    \n",
    "4. **What are the computational benefits of a stride larger than 1.**\n",
    "    Save memory and reduce computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4.5**\n",
    "\n",
    "1. **Assume that we have two convolutional kernels of size k1 and k2 respectively (with no nonlinearity in between).** \n",
    "\n",
    "    • **Prove that the result of the operation can be expressed by a single convolution.** \n",
    "    \n",
    "    Convolution associate law.\n",
    "    \n",
    "    • **What is the dimensionality of the equivalent single convolution?**\n",
    "    \n",
    "    k1+k2-1\n",
    "    \n",
    "    • **Is the converse true?**\n",
    "    \n",
    "    \n",
    "2. **Assume an input shape of ci ×h×w and a convolution kernel with the shape co ×ci ×kh ×kw, padding of (ph,pw), and stride of (sh,sw).**\n",
    "\n",
    "    • **What is the computational cost (multiplications and additions) for the forward computation?** \n",
    "    \n",
    "    • **What is the memory footprint?** \n",
    "    \n",
    "        $O(c_i c_o k_h k_w m_h m_w)$\n",
    "    \n",
    "    • **What is the memory footprint for the backward computation?**\n",
    "    \n",
    "    • **What is the computational cost for the backward computation?**\n",
    "    \n",
    "\thttps://kasperfred.com/posts/computational-complexity-of-neural-networks\n",
    "\n",
    "\n",
    "3. **By what factor does the number of calculations increase if we double the number of input channels ci and the number of output channels co? What happens if we double the padding?**\n",
    "\n",
    "\n",
    "4. **If the height and width of the convolution kernel is $k_h = k_w = 1$, what is the complexity of the forward computation?**\n",
    "\n",
    "        $O(c_i*c_o*h*w)$\n",
    "\n",
    "\n",
    "5. **Are the variables Y1 and Y2 in the last example of this section exactly the same? Why?**\n",
    "\n",
    "    Yes. The main computation of the 1 × 1 convolution occurs on the channel dimension. And $k_h & k_w == 1$ in both function.\n",
    "    \n",
    "    \n",
    "6. **How would you implement convolutions using matrix multiplication when the convolution window is not 1×1 ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.5.5**\n",
    "\n",
    "1. **Implement average pooling as a convolution.**\n",
    "\n",
    "    6.5.1\n",
    "    \n",
    "    \n",
    "2. **What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size  c×h×w , the pooling window has a shape of  ph×pw  with a padding of  $(p_h,p_w)$  and a stride of  $(s_h,s_w)$.**\n",
    "\n",
    "    $c×[(I_h - p_h + pad_h + s_h)/s_h]×[(l_w - p_w + pad_w + s_w)/s_w]$\n",
    "    \n",
    "\n",
    "3. **Why do you expect maximum pooling and average pooling to work differently?**\n",
    "\n",
    "    • Max pooling: the strongest pattern signal in a window\n",
    "    \n",
    "    • Average pooling:  The average signal strength in a window \n",
    "\n",
    "    ![title](image/textbook_solution_6.5.3.png)\n",
    "    \n",
    "    \n",
    "4. **Do we need a separate minimum pooling layer? Can you replace it with another operation?**\n",
    "\n",
    "    $argmin(Xa_{ij}) = argmax( -1 * Xa_{ij})$, hence min-pooling can be modeling through CNN and max-pooling\n",
    "    \n",
    "    \n",
    "5. **Is there another operation between average and maximum pooling that you could consider (hint - recall the softmax)? Why might it not be so popular?**\n",
    "\n",
    "    ?????? \n",
    "    Softmax computation cost is too high.\t\n",
    "    A pooling layer is to alleviate the excessive sensitivity of the convolutional layer to location., i.e. reduce the resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1.5. Exercises**\n",
    "\n",
    "1. **Improve the above model.**\n",
    "\n",
    "    a. **Incorporate more than the past 4 observations? How many do you really need?**\n",
    "    \n",
    "    b. **How many would you need if there were no noise? Hint - you can write  sin  and  cos  as a differential equation.**\n",
    "    \n",
    "    \n",
    "    c.**Can you incorporate older features while keeping the total number of features constant? Does this improve accuracy? Why?**\n",
    "    \n",
    "    \n",
    "    d.**Change the architecture and see what happens.**\n",
    "    \n",
    "    \n",
    "2. **An investor wants to find a good security to buy. She looks at past returns to decide which one is likely to do well. What could possibly go wrong with this strategy?**\n",
    "\n",
    "\n",
    "\n",
    "3. **Does causality also apply to text? To which extent?**\n",
    "\n",
    "\n",
    "\n",
    "4. **Give an example for when a latent variable autoregressive model might be needed to capture the dynamic of the data.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.5. Exercises\n",
    "1. **Suppose there are 100,000 words in the training data set. How many word frequencies and multi-word adjacent frequencies does a four-gram need to store?**\n",
    "\n",
    "In English, the Zipf’s law in the n-gram data exhibits two regimes: one among words with frequencies above about 0.01% (Zipf’s exponent γ ≈ 1) and another (γ ≈ 1.4) among words with\n",
    "frequency below 0.0001%.\n",
    "\n",
    "By Zipf's law, the normalized frequency of elements of rank k, $f(k;s,N)$ is:\n",
    "\n",
    "$$ f(k;s;N) = \\frac{1}{k^s \\sum_1^N \\frac{1}{n^s}},$$ where N = 100,000 is the number of words in the English language, s = 1.07 for unigram. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Review the smoothed probability estimates. Why are they not accurate? Hint - we are dealing with a contiguous sequence rather than singletons.**\n",
    "\n",
    "Laplace smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Lots of contiguous sequence may not occurs in the bag of word, hence the smoothed probability is not accurate in the right tail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **How would you model a dialogue?**\n",
    "\n",
    "First, we can decide which type of dialogue (conversational models) are we going to model (rule-based, retrieval-based, neural generative models, grounded/visual, chit-chat vs. task-based, etc.)?\n",
    "\n",
    "Next, choose framework for modelling, such as for neural generative models choosed Semantically Conditioned LSTM-based model. (https://arxiv.org/abs/1508.01745); or Deep Reinforcement Learning for Dialogue Generation (https://aclweb.org/anthology/D16-1127) and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Estimate the exponent of Zipf’s law for unigrams, bigrams and trigrams.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.5. Exercises\n",
    "\n",
    "1. **If we use an RNN to predict the next character in a text sequence, how many output dimensions do we need?**\n",
    "\n",
    "The output dimension should be the length of the dictionary of unique characters from both train and test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Can you design a mapping for which an RNN with hidden states is exact? Hint - what about a finite number of words?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **What happens to the gradient if you backpropagate through a long sequence?**\n",
    "\n",
    "High powers of matrices can lead to explode or vanish gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **What are some of the problems associated with the simple sequence model described above?**\n",
    "\n",
    " a. numerically unstable;\n",
    " b. difficulty of long-term information preservation and short-term input skipping in latent variable models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4.5. Exercises**\n",
    "\n",
    "1. **What other mini-batch data sampling methods can you think of?**\n",
    "\n",
    "    Non-uniform mini-batch sampling. (i.e. suppressing the probability of similar data points in the same mini-batch, which will reduce the stochastic gradient noise, leading to faster convergence).\n",
    "\n",
    "\n",
    "2. **Why is it a good idea to have a random offset?**\n",
    "    \n",
    "    In this way, we can get both coverage(by sequential partitioning strategies) and randomness.\n",
    "\n",
    "      a. **Does it really lead to a perfectly uniform distribution over the sequences on the document?**\n",
    "\n",
    "        Picking just a random set of initial positions is no good either since it does not guarantee uniform coverage of the array. For instance, if we pick  n  elements at random out of a set of  n  with random replacement, the probability for a particular element not being picked is  \n",
    "            \n",
    "                $$(1−1/n)^n → e^{−1}$$\n",
    "\n",
    "      b. **What would you have to do to make things even more uniform?**\n",
    "\n",
    "            # Offset for the iterator over the data for uniform starts\n",
    "            offset = int(random.uniform(0,num_steps))\n",
    "            \n",
    "            \n",
    "5. **If we want a sequence example to be a complete sentence, what kinds of problems does this introduce in mini-batch sampling? Why would we want to do this anyway?**\n",
    "\n",
    "    Since a complete sentence is long, it is acceptable to discard half-empty mini-batch. Since these sequences are covered by part of other batches in mini-batch sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.8. Exercises**\n",
    "\n",
    "1. **Show that one-hot encoding is equivalent to picking a different embedding for each object.**\n",
    "    \n",
    "    Elementary row and column operations on a matrix are rank-preserving.\n",
    "\n",
    "2. **Adjust the hyperparameters to improve the perplexity.**\n",
    "    \n",
    "    a. **How low can you go? Adjust embeddings, hidden units, learning rate, etc.**\n",
    "    \n",
    "        \n",
    "    \n",
    "    b. **How well will it work on other books by H. G. Wells, e.g. The War of the Worlds.**\n",
    "    \n",
    "    \n",
    "3. **Run the code in this section without clipping the gradient. What happens?**\n",
    "\n",
    "\n",
    "4. **Set the pred_period variable to 1 to observe how the under-trained model (high perplexity) writes lyrics. What can you learn from this?**\n",
    "\n",
    "\n",
    "5. **Change adjacent sampling so that it does not separate hidden states from the computational graph. Does the running time change? How about the accuracy?**\n",
    "\n",
    "\n",
    "6. **Replace the activation function used in this section with ReLU and repeat the experiments in this section.**\n",
    "\n",
    "\n",
    "\n",
    "7. **Prove that the perplexity is the inverse of the harmonic mean of the conditional word probabilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
