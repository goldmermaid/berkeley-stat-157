{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D2L Textbook Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, data as gdata, nn\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4\n",
    "#### 4.1.4. Exercises\n",
    "\n",
    "1. **Compute the derivative of the tanh and the pReLU activation function.**\n",
    "\n",
    "    a. The derivative of the Tanh function is:\n",
    "    $$\\frac{d}{dx} \\mathrm{tanh}(x) = 1 - \\mathrm{tanh}^2(x).$$\n",
    "    b. The derivative of the pReLU function is:    \n",
    "    $$\\mathrm{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)$$\n",
    "    $$\\begin{equation}\n",
    "          \\frac{d}{dx} \\mathrm{pReLU}(x) =\n",
    "                \\begin{cases}\n",
    "                  1 & \\text{if $x > 0$}\\\\\n",
    "                  undefined & \\text{if $x = 0$}\\\\\n",
    "                  \\alpha & \\text{if $x < 0$}\n",
    "                \\end{cases}       \n",
    "        \\end{equation}$$\n",
    "\n",
    "\n",
    "2. **Show that a multilayer perceptron using only ReLU (or pReLU) constructs a continuous piecewise linear function.**\n",
    "\n",
    "    By definiton, a `continous piecewise linear function` is a real-valued function defined on the real numbers, whose graph is composed of continous straight-line sections.\n",
    "    $$\\begin{equation}\n",
    "      \\mathrm{pReLU}(x) =\n",
    "            \\begin{cases}\n",
    "              x & \\text{if $x >= 0$}\\\\\n",
    "              \\alpha x & \\text{if $x < 0$}\n",
    "            \\end{cases}       \n",
    "    \\end{equation}$$\n",
    "    \n",
    "3. **Show that  $tanh(𝑥)+1=2sigmoid(2𝑥)$.**\n",
    "\n",
    "    $$ LHS = \\text{tanh}(x)+1 = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)} + 1\n",
    "                    =\\frac{2}{1 + \\exp(-2x)} .$$\n",
    "    $$ RHS = 2 \\mathrm{sigmoid}(2x) = \\frac{2}{1 + \\exp(-2x)}.$$\n",
    "\n",
    "\n",
    "4. **Assume we have a multilayer perceptron without nonlinearities between the layers. In particular, assume that we have  𝑑  input dimensions,  𝑑  output dimensions and that one of the layers had only  𝑑/2  dimensions. Show that this network is less expressive (powerful) than a single layer perceptron.**\n",
    "\n",
    "    A multilayer perceptron without nonlinearities is equal to one layer perceptron.\n",
    "    $$ {\\hat{\\mathbf{y}}} = \\mathbf{W_2}(\\mathbf{W_1}X + b_1) + b_2 = \\mathbf{W_2} \\mathbf{W_1} X + (\\mathbf{W_2} b_1 + b_2) := \\mathbf{W_3}X + b_3$$\n",
    "    \n",
    "    Hence, if any of layer of d/2 dimension, then the rank of W_3 will be at most d/2, which can not express the final output of dimension d. However, a single layer percptron with sofmax regression will add  nonlinearities to the model, which will learn and represent almost any arbitrary complex function which maps inputs to outputs.\n",
    "    \n",
    "\n",
    "5. **Assume that we have a nonlinearity that applies to one minibatch at a time. What kinds of problems do you expect this to cause?**\n",
    "\n",
    "    Minibatch may not be as representative as whole batch. As a result, parameters learned from the minibatch may get weird gradients and get harder to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7. Exercises¶\n",
    "\n",
    "1. **Change the value of the hyper-parameter `num_hiddens` in order to see how this hyperparameter influences your results.**\n",
    "    \n",
    "    The upper bound on the number of hidden neurons that won't result in over-fitting is:\n",
    "    $$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n",
    "\n",
    "    where $\\alpha$ = an arbitrary scaling factor usually 2-10; $N_i$ = number of input neurons; $N_o$ = number of output neurons; $N_s$ = number of samples in training data set.\n",
    "    \n",
    "   Below the upper bound, the larger the num_hiddens, the better your results might be.\n",
    "    \n",
    "    \n",
    "2. **Try adding a new hidden layer to see how it affects the results.**\n",
    "\n",
    "    In general, adding a new hidden layer to a shallow networks should improve the accuracy. Since wide and shallow networks are very good at memorization, but not so good at generalization. Multiple layers are much better at generalizing because they learn all the intermediate features between the raw data and the high-level classification.\n",
    "    \n",
    "    \n",
    "3. **How does changing the learning rate change the result.**\n",
    "    \n",
    "    If a learning rate is too high, it may overshoot the minimum and fail to converge in the end. If it is too low, then gradient descent can be slow. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Exercises¶\n",
    "\n",
    "1. **Try adding a few more hidden layers to see how the result changes.**\n",
    "\n",
    "2. **Try out different activation functions. Which ones work best?**\n",
    "    \n",
    "    Sigmoid and Tanh both suffers from vanishing gradient problems. \n",
    "    \n",
    "    ReLU rectifies the problem but it could result to \"Dead Neuron\" (since partial of its weights never get updated). Leaky ReLu to fix the problem of dying neurons.  Also, ReLU can be only use within the hidden layer of NN.\n",
    "    \n",
    "    Softmax can be use in the output layers of classification model.\n",
    "    \n",
    "\n",
    "3. **Try out different initializations of the weights.**\n",
    "\n",
    "    Zero initialization. All weights will be the same in the end, since the derivative with respect to loss function is the same.\n",
    "    \n",
    "    Random initialization. Initializing weights randomly, following normal distribution. May suffer from vanishing gradients and exploding gradients.\n",
    "    \n",
    "    Xavier initialization. The initializer fills the weights with random numbers in the range of [−c,c], where $c = \\sqrt{\\frac{3.}{0.5 * (n_{in} + n_{out})}}$. $n_{in}$ is the number of neurons feeding into weights, and $n_{out}$ is the number of neurons the result is fed to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.6. Exercises\n",
    "\n",
    "1. **Can you solve the polynomial regression problem exactly? Hint - use linear algebra.**\n",
    "\n",
    "    Given the polynomial regression samples $\\{(X_i,Y_i)\\}_{i=1}^n$, we have $Y =\\beta_0+\\beta_1X+\\beta_2X^2+\\cdots+\\beta_pX^p+\\varepsilon=\\mathbf{X}\\boldsymbol\\beta+\\varepsilon$\n",
    "        \n",
    "    where\n",
    "    $$\\mathbf{X}=\\pmatrix{\\begin{array}{ccc}1 & X_1 & \\cdots & X_1^p\\\\\n",
    "                \\vdots & \\vdots &\\ddots &\\vdots\\\\\n",
    "                    1 & X_n & \\cdots & X_n^p\n",
    "                \\end{array}}_{n\\times(p+1)}.$$\n",
    "\n",
    "    Hence, $\\hat{\\boldsymbol\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{y},$ where $\\mathbf{y}=(Y_1,\\ldots,Y_n)^T$. \n",
    "    \n",
    "    Note $(𝑝+1)×(𝑝+1) = \\mathrm{rank}(\\mathbf{X}^T\\mathbf{X})=\\mathrm{rank}(\\mathbf{X})=n$. Then, if $p=n−1$, $𝐗^T 𝐗$ has dimension 𝑛×𝑛 and its rank is 𝑛, so no problem, is invertible. But if $p=n$, the dimension of $𝐗^T 𝐗$ is (𝑛+1)×(𝑛+1) and the rank remains 𝑛, so in that case (and also if $p>n$) is not invertible (linear dependence arises).\n",
    "    \n",
    "    \n",
    "2. **Model selection for polynomials**\n",
    "\n",
    "    a. **Plot the training error vs. model complexity (degree of the polynomial). What do you observe?**\n",
    "\n",
    "    b. **Plot the test error in this case.**\n",
    "\n",
    "    c. **Generate the same graph as a function of the amount of data?**\n",
    "\n",
    "    See details in *4.4.4. Polynomial Regression*.\n",
    "    \n",
    "    \n",
    "\n",
    "3. **What happens if you drop the normalization of the polynomial features  $𝑥_𝑖$  by  1/𝑖! . Can you fix this in some other way?**\n",
    "\n",
    "    There might be very large values of gradients and losses, due to very large values for exponents i.\n",
    "\n",
    "\n",
    "4. **What degree of polynomial do you need to reduce the training error to 0?**\n",
    "\n",
    "    As explained in Q1, if $p=n−1$, $𝐗^T 𝐗$ has dimension 𝑛×𝑛 and its rank is 𝑛. Then $\\hat{\\boldsymbol\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{y},$ is the exact answer and hence the training error is 0.\n",
    "    \n",
    "\n",
    "5. **Can you ever expect to see 0 generalization error?**\n",
    "\n",
    "    Yes. Sometimes if we accidentally have training set including all testing set's features and labels, (i.e. testing set items are all duplicated to training set). Then we may see a 0 generalization error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.6. Exercises\n",
    "\n",
    "1. **Experiment with the value of  𝜆  in the estimation problem in this page. Plot training and test accuracy as a function of  𝜆 . What do you observe?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-3:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-2:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in _worker_fn\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e0922625bbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                      wd_list, test_ls, ['train', 'test'])\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mfit_and_plot_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e0922625bbd1>\u001b[0m in \u001b[0;36mfit_and_plot_lambda\u001b[0;34m(wd_list, num_epochs, lr, wd)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fatal error with _push_next, rcvd_idx missing\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_pinned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in <listcomp>\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 128, in __getitem__\n",
      "    return self._fn(*item)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 93, in base_fn\n",
      "    return (fn(x),) + args\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py\", line 53, in forward\n",
      "    x = block(x)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in _worker_fn\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "Process ForkPoolWorker-4:\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in <listcomp>\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 128, in __getitem__\n",
      "    return self._fn(*item)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 917, in forward\n",
      "    return self.hybrid_forward(ndarray, x, *args, **params)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/vision/transforms.py\", line 134, in hybrid_forward\n",
      "    return F.image.to_tensor(x)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 93, in base_fn\n",
      "    return (fn(x),) + args\n",
      "  File \"<string>\", line 27, in to_tensor\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\", line 97, in _imperative_invoke\n",
      "    return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle),\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py\", line 53, in forward\n",
      "    x = block(x)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in _worker_fn\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in <listcomp>\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/ctypes/__init__.py\", line 485, in cast\n",
      "    return _cast(obj, obj, typ)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 917, in forward\n",
      "    return self.hybrid_forward(ndarray, x, *args, **params)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in _worker_fn\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 126, in __getitem__\n",
      "    item = self._data[idx]\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/vision/transforms.py\", line 134, in hybrid_forward\n",
      "    return F.image.to_tensor(x)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 394, in <listcomp>\n",
      "    batch = batchify_fn([_worker_dataset[i] for i in samples])\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 203, in __getitem__\n",
      "    return self._data[idx], self._label[idx]\n",
      "  File \"<string>\", line 27, in to_tensor\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\", line 513, in __getitem__\n",
      "    return self._get_nd_basic_indexing(key)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 128, in __getitem__\n",
      "    return self._fn(*item)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\", line 781, in _get_nd_basic_indexing\n",
      "    shape = self.shape\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\", line 92, in _imperative_invoke\n",
      "    ctypes.byref(out_stypes)))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\", line 93, in base_fn\n",
      "    return (fn(x),) + args\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\", line 1846, in shape\n",
      "    self.handle, ctypes.byref(ndim), ctypes.byref(pdata)))\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py\", line 53, in forward\n",
      "    x = block(x)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 540, in __call__\n",
      "    out = self.forward(*args)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\", line 917, in forward\n",
      "    return self.hybrid_forward(ndarray, x, *args, **params)\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/vision/transforms.py\", line 134, in hybrid_forward\n",
      "    return F.image.to_tensor(x)\n",
      "  File \"<string>\", line 27, in to_tensor\n",
      "  File \"/Users/peanuthu/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\", line 92, in _imperative_invoke\n",
      "    ctypes.byref(out_stypes)))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 10, 0.1, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "\n",
    "def fit_and_plot_lambda(wd_list, num_epochs, lr=0.1, wd=5):\n",
    "    '''\n",
    "    wd_list : a list of number which represents weight_decay value\n",
    "    '''\n",
    "    train_ls, test_ls = [], []\n",
    "    for wd in wd_list:\n",
    "        net = nn.Sequential()\n",
    "        net.add(nn.Dense(1))\n",
    "        net.initialize(init.Normal(sigma=0.1))\n",
    "        trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',\n",
    "                                  {'learning_rate': lr, 'wd': wd})\n",
    "        trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',\n",
    "                                  {'learning_rate': lr})\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            for X, y in train_iter:\n",
    "                with autograd.record():\n",
    "                    l = loss(net(X), y)\n",
    "                l.backward()\n",
    "                # Call the step function on each of the two Trainer instances to\n",
    "                # update the weight and bias separately\n",
    "                trainer_w.step(batch_size)\n",
    "                trainer_b.step(batch_size)\n",
    "        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())\n",
    "        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())\n",
    "    d2l.semilogy(wd_list, train_ls, 'weight_decay', 'loss',\n",
    "                     wd_list, test_ls, ['train', 'test'])\n",
    "    \n",
    "fit_and_plot_lambda(wd_list=range(10), num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Use a validation set to find the optimal value of  𝜆 . Is it really the optimal value? Does this matter?**\n",
    "\n",
    "    Given different hyperparameters, network architecture and dataset, the optimal weight decay will vary. Hence there is no global optimal value. \n",
    "\n",
    "\n",
    "3. **What would the update equations look like if instead of  $‖𝐰‖^2$  we used  $∑_𝑖|𝑤_𝑖|$  as our penalty of choice (this is called  ℓ1  regularization).**\n",
    "\n",
    "    For L2, the loss function and corresponding stochastic gradient descent updates is :\n",
    "    $$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
    "    $$\\begin{aligned}\n",
    "w & \\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
    "\\end{aligned}$$\n",
    "\n",
    "    For L1, the loss function and corresponding stochastic gradient descent updates is :\n",
    "    $$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} ∑_𝑖|𝑤_𝑖|$$\n",
    "    $$\\begin{aligned}\n",
    "w & \\leftarrow \\left(1- \\frac{\\eta\\lambda}{2 |\\mathcal{B}| \\mathbf{|w|}} \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "4. **We know that  $‖𝐰‖^2=𝐰^⊤𝐰$ . Can you find a similar equation for matrices (mathematicians call this the Frobenius norm)?**\n",
    "\n",
    "    ![title](image/textbook_solution_4.5.6.png)\n",
    "    \n",
    "    \n",
    "5. **Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways can you think of to deal with overfitting?**\n",
    "\n",
    "    Add more training examples; Increase dropout; Decrease features, etc.\n",
    "    \n",
    "\n",
    "6. **In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via  $𝑝(𝑤|𝑥)∝𝑝(𝑥|𝑤)𝑝(𝑤)$ . How can you identify  𝑝(𝑤)  with regularization?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.7. Exercises\n",
    "\n",
    "1. **Try out what happens if you change the dropout probabilities for layers 1 and 2. In particular, what happens if you switch the ones for both layers?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## network 1\n",
    "net467_1 = nn.Sequential()\n",
    "net467_1.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net467_1.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_1.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_1, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## network 2, switch dropout rate\n",
    "net467_2 = nn.Sequential()\n",
    "net467_2.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10))\n",
    "net467_2.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_2.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_2, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Increase the number of epochs and compare the results obtained when using dropout with those when not using it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## network 3\n",
    "num_epochs = 50\n",
    "net467_3 = nn.Sequential()\n",
    "net467_3.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10))\n",
    "net467_3.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net467_3.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net467_3, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compute the variance of the the activation random variables after applying dropout.**\n",
    "\n",
    "    Dropout replaces an activation  ℎ  with a random variable  ℎ′  with expected value  ℎ  and with variance given by the dropout probability  𝑝 .\n",
    "    \n",
    "    $$\\begin{split}\\begin{aligned}\n",
    "        h' =\n",
    "        \\begin{cases}\n",
    "            0 & \\text{ with probability } p \\\\\n",
    "            \\frac{h}{1-p} & \\text{ otherwise}\n",
    "        \\end{cases}\n",
    "        \\end{aligned}\\end{split}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Why should you typically not using dropout?**\n",
    "\n",
    "    Dropout can help with regularization, but at a risk of lossing improtant information. Especially applying Dropout in the first layer will lead to significant inforamtion loss.\n",
    "    \n",
    "\n",
    "5. **If changes are made to the model to make it more complex, such as adding hidden layer units, will the effect of using dropout to cope with overfitting be more obvious?**\n",
    "   \n",
    "   For an overfitted model, adding a hidden layer with dropout may not help. Especially in the situation that the effective neurons in this layer is larger than the number of neurons in the later layers, since this is equal to adding an extra hidden layer. \n",
    "   \n",
    "\n",
    "6. **Using the model in this section as an example, compare the effects of using dropout and weight decay. What if dropout and weight decay are used at the same time?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Run on GPU\n",
    "\n",
    "## random simulate dataset\n",
    "n_train, n_test, num_inputs = 20, 100, 200\n",
    "true_w, true_b = nd.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "\n",
    "features = nd.random.normal(shape=(n_train + n_test, num_inputs))\n",
    "labels = nd.dot(features, true_w) + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)\n",
    "train_features, test_features = features[:n_train, :], features[n_train:, :]\n",
    "train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(train_features, train_labels), batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_plot_gluon_467_6(dropout, wd, num_epochs=50, lr=0.01, batch_size=256):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Dense(10))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "\n",
    "    trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',\n",
    "                              {'learning_rate': lr, 'wd': wd})\n",
    "    # The bias parameter has not decayed. Bias names generally end with \"bias\"\n",
    "    trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',\n",
    "                              {'learning_rate': lr})\n",
    "    train_ls, test_ls = [], []\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            # Call the step function on each of the two Trainer instances to\n",
    "            # update the weight and bias separately\n",
    "            trainer_w.step(batch_size)\n",
    "            trainer_b.step(batch_size)\n",
    "        train_ls.append(loss(net(train_features),\n",
    "                             train_labels).mean().asscalar())\n",
    "        test_ls.append(loss(net(test_features),\n",
    "                            test_labels).mean().asscalar())\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n",
    "                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n",
    "#     print('L2 norm of w:', net[0].weight.data().norm().asscalar())\n",
    "\n",
    "fit_and_plot_gluon_467_6(dropout=0.5, wd=0) \n",
    "fit_and_plot_gluon_467_6(dropout=0.5, wd=3)\n",
    "fit_and_plot_gluon_467_6(dropout=0, wd=3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. **What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?**\n",
    "\n",
    "    The regularization effect will be the same. If we turn partial of the weights to be zero, these neurons won't learn any signal from the inputs, which will have the similar functionality as dropout on activation.\n",
    "\n",
    "\n",
    "8. **Replace the dropout activation with a random variable that takes on values of  $[0,\\gamma/2,\\gamma]$ . Can you design something that works better than the binary dropout function? Why might you want to use it? Why not?**\n",
    "\n",
    "    Define the following dropout activation function:\n",
    "\n",
    "    $$\\begin{split}\\begin{aligned}\n",
    "    h' =\n",
    "    \\begin{cases}\n",
    "        0 & \\text{ with probability } p_1 \\\\\n",
    "        \\gamma/2 * h & \\text{ with probability } (1 - p_1)  p_2 \\\\\n",
    "        \\gamma * h & \\text{ with probability } (1 - p_1)(1 - p_2)\n",
    "    \\end{cases}\n",
    "    \\end{aligned}\\end{split}$$\n",
    "\n",
    "    The has the expectation remained unchanged, we need to have\n",
    "    $$ 0 + (\\gamma/2) h (1 - p_1)  p_2 +  \\gamma  h  (1 - p_1)(1 - p_2)  = h$$\n",
    "    Thus,\n",
    "     $$\\gamma = \\frac{2}{(2-p_2)(1-p_1)}$$\n",
    "\n",
    "    For example, if we let $p_1=0.2, p_2=0.75$, then $\\gamma = 2$ by the above formula,\n",
    "    \n",
    "     $$\\begin{split}\\begin{aligned}\n",
    "    h' =\n",
    "    \\begin{cases}\n",
    "        0 & \\text{ with probability } 0.2 \\\\\n",
    "        h & \\text{ with probability } 0.6 \\\\\n",
    "        2 h & \\text{ with probability } 0.2\n",
    "    \\end{cases}\n",
    "    \\end{aligned}\\end{split}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.6. Exercises\n",
    "\n",
    "1. **Assume that the inputs  X  are matrices. What is the dimensionality of the gradients?**\n",
    "\n",
    "    Notice the dimensionality of each layer's graditents is equal the dimensionality of each layer's weighs. i.e. \n",
    "    $$ dim(\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}) = dim(\\mathbf{W}^{(1)})$$\n",
    "    \n",
    "    If the inputs $X \\in \\mathbb{R}^{d \\times c}$ are matrices at each row, the weight matrix $\\mathbf{W}^{(1)}$ would have a dimension equal to ${h \\times (d \\times  c)}$, where h is the hidden layer dimension.\n",
    "    \n",
    "\n",
    "2. **Add a bias to the hidden layer of the model described in this chapter.**\n",
    "    a. **Draw the corresponding compute graph.**\n",
    "    b. **Derive the forward and backward propagation equations.**\n",
    "    \n",
    "    b. Given a hidden layer with a bias:\n",
    "    $$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\\\\n",
    "      \\mathbf{h}= \\phi (\\mathbf{z}) \\\\\n",
    "      \\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)}\\\\\n",
    "      L = l(\\mathbf{o}, y) \\\\\n",
    "      s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) \\\\\n",
    "      J = L + s$$\n",
    "      \n",
    "      hence the backward propagation will be\n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}\n",
    "        = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$$\n",
    "        \n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}}\n",
    "        = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{b}^{(2)}}\\right)  + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{b}^{(2)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\times 1 + 0\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}}$$\n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}} + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} {\\mathbf{W}^{(2)}}^\\top \\odot \\phi'\\left(\\mathbf{z}\\right) {\\mathbf{x}}^\\top + \\lambda \\mathbf{W}^{(1)}$$\n",
    "        \n",
    "      $$ \\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}^{(1)}} + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{b}^{(1)}}\\right)\n",
    "        = \\frac{\\partial J}{\\partial \\mathbf{o}} {\\mathbf{W}^{(2)}}^\\top \\odot \\phi'\\left(\\mathbf{z}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Compute the memory footprint for training and inference in model described in the current chapter.**\n",
    "\n",
    "    Training: need memory for $$ \\frac{\\partial J}{\\partial \\mathbf{o}}, {\\mathbf{W}^{(2)}}, {\\mathbf{W}^{(1)}}, \\phi'\\left(\\mathbf{z}\\right), {\\mathbf{x}},$$\n",
    "    Inference: only need memory for $$\\mathbf{W}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(1)}, \\mathbf{b}^{(2)}$$\n",
    "\n",
    "\n",
    "4. **Assume that you want to compute second derivatives. What happens to the compute graph? Is this a good idea?**\n",
    "\n",
    "    \n",
    "\n",
    "5. **Assume that the compute graph is too large for your GPU.**\n",
    "\n",
    "    a.**Can you partition it over more than one GPU?**\n",
    "        \n",
    "        By default, MXNet uses data parallelism to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices.\n",
    "        \n",
    "    b.**What are the advantages and disadvantages over training on a smaller minibatch?**\n",
    "    \n",
    "    Advantages:\n",
    "        1. More robust convergence, avoiding local minima (as its model update frequency is higher than batch gradient descent.)\n",
    "        2. Computationally efficient process than stochastic gradient descent.\n",
    "        3. Memory efficient than batch gradient descent.\n",
    "        \n",
    "    Disadvantages:\n",
    "        1. Tune minibatch hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4. Exercises\n",
    "\n",
    "1. **Can you design other cases of n,fjbhbf?**\n",
    "\n",
    "\n",
    "2. **Can we initialize all weight parameters in linear regression or in softmax regression to the same value?**\n",
    "\n",
    "    When all weights initialized to the same value, the weights will update with exact same gradient all the time. Only when initializing the model to small random values breaks the symmetry and allows different weights to learn independently of each other.\n",
    "\n",
    "\n",
    "3. **Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?**\n",
    "\n",
    "    To compute the overall error gradient,\n",
    "$$% <![CDATA[\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\textbf{h}_{t}}{\\partial \\textbf{h}_{k}} &= \\frac{\\partial \\textbf{h}_{t}}{\\partial \\textbf{h}_{t-1}} \\frac{\\partial \\textbf{h}_{t-1}}{\\partial \\textbf{h}_{t-2}} \\cdots \\frac{\\partial \\textbf{h}_{k+1}}{\\partial \\textbf{h}_{k}} \\\\\n",
    "&= \\prod_{i=k+1}^{t} \\frac{\\partial \\textbf{h}_{i}}{\\partial \\textbf{h}_{i-1}} \\\\\n",
    "&= \\prod_{i=k+1}^{t} \\textbf{W}^\\top \\text{diag} \\left[ f'\\left(\\textbf{h}_{i-1}\\right) \\right] \n",
    "\\end{align} %]]>$$\n",
    "\n",
    "    At each time step t,\n",
    "$$\\begin{align}\n",
    "\\left\\lVert \\frac{\\partial \\textbf{h}_{i}}{\\partial \\textbf{h}_{i-1}} \\right\\rVert \\leq \\left\\lVert \\textbf{W}^\\top \\right\\rVert \\left\\lVert \\text{diag} \\left[ f'\\left(\\textbf{h}_{i-1}\\right) \\right] \\right\\rVert \\leq \\gamma_{\\textbf{W}} \\gamma_{\\textbf{h}} \\tag{11}\n",
    "\\end{align}$$\n",
    "\n",
    "    where define $\\gamma_{\\textbf{W}}$ as the largest eigenvalue associated with $‖W^T‖$, as its upper bound, and $\\gamma_{\\textbf{h}}$ as the largest eigenvalue associated with $\\left\\lVert \\text{diag} \\left[ f'\\left(\\textbf{h}_{i-1}\\right) \\right] \\right\\rVert$. Notice that for $tanh$ we have $\\gamma_{\\textbf{h}}=1$ while for $sigmoid$ we have $\\gamma_{\\textbf{h}}=1/4$. Hence, after t steps,\n",
    "$$\\begin{align}\n",
    "\\left\\lVert \\frac{\\partial \\textbf{h}_{t}}{\\partial \\textbf{h}_{k}} \\right\\rVert = \\left\\lVert \\prod_{i=k+1}^{t} \\frac{\\partial \\textbf{h}_{i}}{\\partial \\textbf{h}_{i-1}} \\right\\rVert \\leq  \\left( \\gamma_{\\textbf{W}} \\gamma_{\\textbf{h}} \\right)^{t-k} \\tag{12}\n",
    "\\end{align}$$\n",
    "\n",
    "    As a result, $\\gamma_{\\textbf{W}}\\gamma_{\\textbf{h}} < 1$  the gradient tends to vanish while for $\\gamma_{\\textbf{W}}\\gamma_{\\textbf{h}} > 1$ the gradient tends to explode.\n",
    "    \n",
    "\n",
    "4. **If we know that some terms diverge, can we fix this after the fact? Look at the paper on LARS by You, Gitman and Ginsburg, 2017 for inspiration.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.5. Exercises\n",
    "\n",
    "1. **What could happen when we change the behavior of a search engine? What might the users do? What about the advertisers?**\n",
    "\n",
    "2. **Implement a covariate shift detector. Hint - build a classifier.**\n",
    "\n",
    "\n",
    "3. **Implement a covariate shift corrector.**\n",
    "\n",
    "\n",
    "4. **What could go wrong if training and test set are very different? What would happen to the sample weights?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
